{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf63591",
   "metadata": {},
   "source": [
    "Задача - суммаризация текста научных статей\n",
    "\n",
    "Данные - оказывается данные с ArXiv есть в TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6dd76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e9ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder  = tfds.builder('scientific_papers/arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d20bf9d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\v.shirobokov\\tensorflow_datasets\\scientific_papers\\arxiv\\1.1.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04c3cab0da849349779e0c527516581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fda159cb584fbb9eac983237d56844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc2704528e8412a865578d414173efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3299cf8e5b147fdb08cccd4440b787a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 1 column 89060 (char 89059)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Скачать если не скачан предварительно (примерно 4 Гб(архив) + 15Гб окончательный размер на диске)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Возможна проблема при экстракции данных из-за недостатка оперативки. 12.7Гб на колабе не хватило\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Я использовал факультетский кластер на 1Тб оперативки, в максимуме что я заметил, он требовал > 24Гб\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\wrapt\\wrappers.py:644\u001b[0m, in \u001b[0;36mBoundFunctionWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         wrapped \u001b[38;5;241m=\u001b[39m PartialCallableObjectProxy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__, instance)\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_wrapper(wrapped, instance, args, kwargs)\n\u001b[1;32m--> 644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_self_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_self_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;66;03m# As in this case we would be dealing with a classmethod or\u001b[39;00m\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;66;03m# staticmethod, then _self_instance will only tell us whether\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;66;03m# class type, as it reflects what they have available in the\u001b[39;00m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;66;03m# decoratored function.\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:646\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    644\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir)\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 646\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    652\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    653\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    654\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1535\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1523\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m split_name, generator \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   1524\u001b[0m       split_generators\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1525\u001b[0m       desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating splits...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1526\u001b[0m       unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m splits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1527\u001b[0m       leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1528\u001b[0m   ):\n\u001b[0;32m   1529\u001b[0m     filename_template \u001b[38;5;241m=\u001b[39m naming\u001b[38;5;241m.\u001b[39mShardedFileTemplate(\n\u001b[0;32m   1530\u001b[0m         split\u001b[38;5;241m=\u001b[39msplit_name,\n\u001b[0;32m   1531\u001b[0m         dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   1532\u001b[0m         data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[0;32m   1533\u001b[0m         filetype_suffix\u001b[38;5;241m=\u001b[39mpath_suffix,\n\u001b[0;32m   1534\u001b[0m     )\n\u001b[1;32m-> 1535\u001b[0m     future \u001b[38;5;241m=\u001b[39m \u001b[43msplit_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_split_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1541\u001b[0m     split_info_futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;66;03m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\split_builder.py:341\u001b[0m, in \u001b[0;36mSplitBuilder.submit_split_generation\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Depending on the type of generator, we use the corresponding\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# `_build_from_xyz` method.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generator, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[1;32m--> 341\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_from_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Otherwise, beam required\u001b[39;00m\n\u001b[0;32m    343\u001b[0m   unknown_generator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    344\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid split generator value for split `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    345\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected generator or apache_beam object. Got: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    346\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(generator)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    347\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\split_builder.py:406\u001b[0m, in \u001b[0;36mSplitBuilder._build_from_generator\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    396\u001b[0m serialized_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\u001b[38;5;241m.\u001b[39mget_serialized_info()\n\u001b[0;32m    397\u001b[0m writer \u001b[38;5;241m=\u001b[39m writer_lib\u001b[38;5;241m.\u001b[39mWriter(\n\u001b[0;32m    398\u001b[0m     serializer\u001b[38;5;241m=\u001b[39mexample_serializer\u001b[38;5;241m.\u001b[39mExampleSerializer(serialized_info),\n\u001b[0;32m    399\u001b[0m     filename_template\u001b[38;5;241m=\u001b[39mfilename_template,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m     shard_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shard_config,\n\u001b[0;32m    405\u001b[0m )\n\u001b[1;32m--> 406\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m    407\u001b[0m     generator,\n\u001b[0;32m    408\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples...\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    409\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    410\u001b[0m     total\u001b[38;5;241m=\u001b[39mtotal_num_examples,\n\u001b[0;32m    411\u001b[0m     leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    412\u001b[0m ):\n\u001b[0;32m    413\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\u001b[38;5;241m.\u001b[39mencode_example(example)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\datasets\\scientific_papers\\scientific_papers_dataset_builder.py:107\u001b[0m, in \u001b[0;36mBuilder._generate_examples\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epath\u001b[38;5;241m.\u001b[39mPath(path)\u001b[38;5;241m.\u001b[39mopen() \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    100\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Possible keys are:\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# \"article_id\": str\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# \"section_names\": list[str], list of section names.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# \"sections\": list[list[str]], list of sections (list of paragraphs)\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# In original paper, <S> and </S> are not used in vocab during training\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# or during decoding.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# https://github.com/armancohan/long-summarization/blob/master/data.py#L27\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 1 column 89060 (char 89059)"
     ]
    }
   ],
   "source": [
    "# Скачать если не скачан предварительно (примерно 4 Гб(архив) + 15Гб окончательный размер на диске)\n",
    "# Возможна проблема при экстракции данных из-за недостатка оперативки. 12.7Гб на колабе не хватило\n",
    "# Я использовал факультетский кластер на 1Тб оперативки, в максимуме что я заметил, он требовал > 24Гб\n",
    "builder.download_and_prepare()\n",
    "# Однако даже после скачивания builder подготавливает данные с ошибкой, \n",
    "# вроде дело в парсере json ,но на дебаг просто нет времени\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57ae41",
   "metadata": {},
   "source": [
    "# Начало тутъ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69cc37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попытка номер 2 - HuggingFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189c6005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scientific_papers (C:/Users/v.shirobokov/.cache/huggingface/datasets/scientific_papers/arxiv/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8e8c53cb4746b5986409e82d18c953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"scientific_papers\", 'arxiv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de61e1",
   "metadata": {},
   "source": [
    "Тут датасет удобно разбит на выборки. Сами выборки сразу поделены на абстракт и статью. Section names в данной задаче суммаризации я пока не вижу смысла использовать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502dfcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 203037\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 6436\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 6440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b196fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dfa910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d9ad3",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43dc63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8db1f1",
   "metadata": {},
   "source": [
    "Взял один экземпляр чтобы отладить часть кода с отчисткой и с токенизацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df40c9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'introduction\\nmethods of periodicity analysis\\na method of the diagnosis of an echo-effect in the power spectrum\\ndata analysis\\nthe periodicity at about 155 days during the maximum activity period\\nconclusion\\nacknowledgments'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token = test_dataset[0].copy()\n",
    "test_token['section_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1780b7",
   "metadata": {},
   "source": [
    "Проверял как быстро работает spacy на одном абстракте "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfdb10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b2b396a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.78 s\n",
      "Wall time: 6.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 's',\n",
       " 'h',\n",
       " 'o',\n",
       " 'r',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'm',\n",
       " '',\n",
       " 'p',\n",
       " 'e',\n",
       " 'r',\n",
       " '',\n",
       " 'o',\n",
       " 'd',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'e',\n",
       " 's',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " '',\n",
       " 'l',\n",
       " 'y',\n",
       " '',\n",
       " 's',\n",
       " 'u',\n",
       " 'n',\n",
       " 's',\n",
       " 'p',\n",
       " 'o',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " 'r',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " 'f',\n",
       " 'l',\n",
       " 'u',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " '',\n",
       " 'f',\n",
       " 'r',\n",
       " 'o',\n",
       " 'm',\n",
       " '',\n",
       " '',\n",
       " 'u',\n",
       " 'g',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 't',\n",
       " 'o',\n",
       " '',\n",
       " 'o',\n",
       " 'c',\n",
       " 't',\n",
       " 'o',\n",
       " 'b',\n",
       " 'e',\n",
       " 'r',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'r',\n",
       " 'e',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " 's',\n",
       " 'c',\n",
       " 'u',\n",
       " 's',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 's',\n",
       " 'e',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'c',\n",
       " 'o',\n",
       " 'r',\n",
       " 'r',\n",
       " 'e',\n",
       " 'l',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'v',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " '',\n",
       " 'l',\n",
       " 'y',\n",
       " 's',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 'd',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " '',\n",
       " 'n',\n",
       " 'e',\n",
       " 'g',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'v',\n",
       " 'e',\n",
       " '',\n",
       " 'c',\n",
       " 'o',\n",
       " 'r',\n",
       " 'r',\n",
       " 'e',\n",
       " 'l',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'o',\n",
       " 'n',\n",
       " '',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'p',\n",
       " 'e',\n",
       " 'r',\n",
       " '',\n",
       " 'o',\n",
       " 'd',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " 't',\n",
       " 'y',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " '',\n",
       " 'b',\n",
       " 'o',\n",
       " 'u',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " 'x',\n",
       " 'm',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " '',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " 'y',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'b',\n",
       " 'u',\n",
       " 't',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'p',\n",
       " 'o',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " '',\n",
       " 's',\n",
       " 'p',\n",
       " 'e',\n",
       " 'c',\n",
       " 't',\n",
       " 'r',\n",
       " 'u',\n",
       " 'm',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " '',\n",
       " 'l',\n",
       " 'y',\n",
       " 's',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 'd',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 's',\n",
       " 't',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 's',\n",
       " 't',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " 'l',\n",
       " 'l',\n",
       " 'y',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " 'g',\n",
       " 'n',\n",
       " '',\n",
       " 'f',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " 'n',\n",
       " 't',\n",
       " '',\n",
       " 'p',\n",
       " 'e',\n",
       " '',\n",
       " 'k',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'm',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'v',\n",
       " '',\n",
       " 'l',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 'e',\n",
       " 'w',\n",
       " '',\n",
       " 'm',\n",
       " 'e',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'd',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " '',\n",
       " 'g',\n",
       " 'n',\n",
       " 'o',\n",
       " 's',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " '',\n",
       " 'e',\n",
       " 'c',\n",
       " 'h',\n",
       " 'o',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'e',\n",
       " 'f',\n",
       " 'f',\n",
       " 'e',\n",
       " 'c',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " '',\n",
       " 's',\n",
       " 'p',\n",
       " 'e',\n",
       " 'c',\n",
       " 't',\n",
       " 'r',\n",
       " 'u',\n",
       " 'm',\n",
       " '',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'p',\n",
       " 'o',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 'd',\n",
       " '',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " 's',\n",
       " 't',\n",
       " '',\n",
       " 't',\n",
       " 'e',\n",
       " 'd',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " 'y',\n",
       " '',\n",
       " 'p',\n",
       " 'e',\n",
       " 'r',\n",
       " '',\n",
       " 'o',\n",
       " 'd',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " 't',\n",
       " 'y',\n",
       " '',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'h',\n",
       " '',\n",
       " 'r',\n",
       " 'm',\n",
       " 'o',\n",
       " 'n',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'p',\n",
       " 'e',\n",
       " 'r',\n",
       " '',\n",
       " 'o',\n",
       " 'd',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'e',\n",
       " 's',\n",
       " '',\n",
       " 'f',\n",
       " 'r',\n",
       " 'o',\n",
       " 'm',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'v',\n",
       " '',\n",
       " 'l',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " '',\n",
       " 'x',\n",
       " 'm',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " '',\n",
       " '',\n",
       " '$',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " 'y',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " 'u',\n",
       " 't',\n",
       " 'o',\n",
       " 'c',\n",
       " 'o',\n",
       " 'r',\n",
       " 'r',\n",
       " 'e',\n",
       " 'l',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'o',\n",
       " 'n',\n",
       " '',\n",
       " 'f',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " '',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " '',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " '',\n",
       " 'l',\n",
       " 'y',\n",
       " '',\n",
       " 's',\n",
       " 'u',\n",
       " 'n',\n",
       " 's',\n",
       " 'p',\n",
       " 'o',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " 'r',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " 'f',\n",
       " 'l',\n",
       " 'u',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 'd',\n",
       " '',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'f',\n",
       " 'l',\n",
       " 'u',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'o',\n",
       " 'n',\n",
       " 'e',\n",
       " '',\n",
       " 'r',\n",
       " 'o',\n",
       " 't',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'o',\n",
       " 'n',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'm',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'v',\n",
       " '',\n",
       " 'l',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'n',\n",
       " '',\n",
       " 'h',\n",
       " 'e',\n",
       " 'm',\n",
       " '',\n",
       " 's',\n",
       " 'p',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 's',\n",
       " 'e',\n",
       " 'p',\n",
       " '',\n",
       " 'r',\n",
       " '',\n",
       " 't',\n",
       " 'e',\n",
       " 'l',\n",
       " 'y',\n",
       " '',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'w',\n",
       " 'h',\n",
       " 'o',\n",
       " 'l',\n",
       " 'e',\n",
       " '',\n",
       " 's',\n",
       " 'o',\n",
       " 'l',\n",
       " '',\n",
       " 'r',\n",
       " '',\n",
       " 'c',\n",
       " 'y',\n",
       " 'c',\n",
       " 'l',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 'd',\n",
       " '',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'm',\n",
       " '',\n",
       " 'x',\n",
       " '',\n",
       " 'm',\n",
       " 'u',\n",
       " 'm',\n",
       " '',\n",
       " '',\n",
       " 'c',\n",
       " 't',\n",
       " '',\n",
       " 'v',\n",
       " '',\n",
       " 't',\n",
       " 'y',\n",
       " '',\n",
       " 'p',\n",
       " 'e',\n",
       " 'r',\n",
       " '',\n",
       " 'o',\n",
       " 'd',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " 'c',\n",
       " 'y',\n",
       " 'c',\n",
       " 'l',\n",
       " 'e',\n",
       " '',\n",
       " 'd',\n",
       " 'o',\n",
       " '',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " '',\n",
       " 's',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " 'f',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'e',\n",
       " 's',\n",
       " 'p',\n",
       " 'e',\n",
       " 'c',\n",
       " '',\n",
       " '',\n",
       " 'l',\n",
       " 'l',\n",
       " 'y',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'v',\n",
       " '',\n",
       " 'l',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " '',\n",
       " 'x',\n",
       " 'm',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " '',\n",
       " '',\n",
       " '$',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " 'y',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'v',\n",
       " 'e',\n",
       " 's',\n",
       " '',\n",
       " '',\n",
       " 'g',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 's',\n",
       " '',\n",
       " 's',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'e',\n",
       " 'x',\n",
       " '',\n",
       " 's',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'o',\n",
       " 'n',\n",
       " 'g',\n",
       " '',\n",
       " 'p',\n",
       " 'o',\n",
       " 's',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'v',\n",
       " 'e',\n",
       " '',\n",
       " 'f',\n",
       " 'l',\n",
       " 'u',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " '',\n",
       " 'o',\n",
       " 'f',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " '',\n",
       " 'b',\n",
       " 'o',\n",
       " 'u',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " 'x',\n",
       " 'm',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " '',\n",
       " '',\n",
       " 'd',\n",
       " '',\n",
       " 'y',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'v',\n",
       " '',\n",
       " 'l',\n",
       " '',\n",
       " '',\n",
       " 'n',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'm',\n",
       " '',\n",
       " 'x',\n",
       " '',\n",
       " 'm',\n",
       " 'u',\n",
       " 'm',\n",
       " '',\n",
       " '',\n",
       " 'c',\n",
       " 't',\n",
       " '',\n",
       " 'v',\n",
       " '',\n",
       " 't',\n",
       " 'y',\n",
       " '',\n",
       " 'p',\n",
       " 'e',\n",
       " 'r',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_test_data = map(\n",
    "    lambda x: ' '.join(\n",
    "        token.lemma_.lower() for token in nlp(x) if \n",
    "        not token.is_stop \n",
    "        and not token.is_punct\n",
    "        and not token.is_digit\n",
    "        and not token.like_email\n",
    "        and not token.like_num\n",
    "        and not token.is_space\n",
    "    ), test_token['abstract']\n",
    ")\n",
    "list(test_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478bb32",
   "metadata": {},
   "source": [
    "На мой взгляд, как-то долго для одного только абстракта. \n",
    "\n",
    "Попробую почистить через регулярные выражения значения \\n, $ ,@xmath, @xcite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3000d381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for about 20 years the problem of properties of short - term changes of solar activity has been considered extensively .\\nmany investigators studied the short - term periodicities of the various indices of solar activity .\\nseveral periodicities were detected , but the periodicities about 155 days and from the interval of @xmath3 $ ] days ( @xmath4 $ ] years ) are mentioned most often .\\nfirst of them was discovered by @xcite in the occurence rate of gamma - ray flares detected by the gamma - ray spectrometer aboard the _ solar maximum mission ( smm ) .\\nthis periodicity was confirmed for other solar flares data and for the same time period @xcite .\\nit was also found in proton flares during solar cycles 19 and 20 @xcite , but it was not found in the solar flares data during solar cycles 22 @xcite .\\n_    several autors confirmed above results for the daily sunspot area data . @xcite studied the sunspot data from 18741984 .\\nshe found the 155-day periodicity in data records from 31 years .\\nthis periodicity is always characteristic for one of the solar hemispheres ( the southern hemisphere for cycles 1215 and the northern hemisphere for cycles 1621 ) .\\nmoreover , it is only present during epochs of maximum activity ( in episodes of 13 years ) .\\nsimilarinvestigationswerecarriedoutby + @xcite .\\nthey applied the same power spectrum method as lean , but the daily sunspot area data ( cycles 1221 ) were divided into 10 shorter time series .\\nthe periodicities were searched for the frequency interval 57115 nhz ( 100200 days ) and for each of 10 time series .\\nthe authors showed that the periodicity between 150160 days is statistically significant during all cycles from 16 to 21 .\\nthe considered peaks were remained unaltered after removing the 11-year cycle and applying the power spectrum analysis .\\n@xcite used the wavelet technique for the daily sunspot areas between 1874 and 1993 .\\nthey determined the epochs of appearance of this periodicity and concluded that it presents around the maximum activity period in cycles 16 to 21 .\\nmoreover , the power of this periodicity started growing at cycle 19 , decreased in cycles 20 and 21 and disappered after cycle 21 .\\nsimilaranalyseswerepresentedby + @xcite , but for sunspot number , solar wind plasma , interplanetary magnetic field and geomagnetic activity index @xmath5 .\\nduring 1964 - 2000 the sunspot number wavelet power of periods less than one year shows a cyclic evolution with the phase of the solar cycle.the 154-day period is prominent and its strenth is stronger around the 1982 - 1984 interval in almost all solar wind parameters .\\nthe existence of the 156-day periodicity in sunspot data were confirmed by @xcite .\\nthey considered the possible relation between the 475-day ( 1.3-year ) and 156-day periodicities .\\nthe 475-day ( 1.3-year ) periodicity was also detected in variations of the interplanetary magnetic field , geomagnetic activity helioseismic data and in the solar wind speed @xcite .\\n@xcite concluded that the region of larger wavelet power shifts from 475-day ( 1.3-year ) period to 620-day ( 1.7-year ) period and then back to 475-day ( 1.3-year ) .\\nthe periodicities from the interval @xmath6 $ ] days ( @xmath4 $ ] years ) have been considered from 1968 .\\n@xcite mentioned a 16.3-month ( 490-day ) periodicity in the sunspot numbers and in the geomagnetic data .\\n@xcite analysed the occurrence rate of major flares during solar cycles 19 .\\nthey found a 18-month ( 540-day ) periodicity in flare rate of the norhern hemisphere .\\n@xcite confirmed this result for the @xmath7 flare data for solar cycles 20 and 21 and found a peak in the power spectra near 510540 days .\\n@xcite found a 17-month ( 510-day ) periodicity of sunspot groups and their areas from 1969 to 1986 .\\nthese authors concluded that the length of this period is variable and the reason of this periodicity is still not understood .\\n@xcite and + @xcite obtained statistically significant peaks of power at around 158 days for daily sunspot data from 1923 - 1933 ( cycle 16 ) . in this paper the problem of the existence of this periodicity for sunspot data from cycle 16 is considered .\\nthe daily sunspot areas , the mean sunspot areas per carrington rotation , the monthly sunspot numbers and their fluctuations , which are obtained after removing the 11-year cycle are analysed . in section 2 the properties of the power spectrum methods are described . in section 3 a new approach to the problem of aliases in the power spectrum analysis\\nis presented . in section 4 numerical results of the new method of the diagnosis of an echo - effect for sunspot area data are discussed . in section 5 the problem of the existence of the periodicity of about 155 days during the maximum activity period for sunspot data from the whole solar disk and from each solar hemisphere separately is considered .\\nto find periodicities in a given time series the power spectrum analysis is applied . in this paper\\ntwo methods are used : the fast fourier transformation algorithm with the hamming window function ( fft ) and the blackman - tukey ( bt ) power spectrum method @xcite .\\nthe bt method is used for the diagnosis of the reasons of the existence of peaks , which are obtained by the fft method .\\nthe bt method consists in the smoothing of a cosine transform of an autocorrelation function using a 3-point weighting average .\\nsuch an estimator is consistent and unbiased .\\nmoreover , the peaks are uncorrelated and their sum is a variance of a considered time series . the main disadvantage of this method is a weak resolution of the periodogram points , particularly for low frequences .\\nfor example , if the autocorrelation function is evaluated for @xmath8 , then the distribution points in the time domain are : @xmath9 thus , it is obvious that this method should not be used for detecting low frequency periodicities with a fairly good resolution .\\nhowever , because of an application of the autocorrelation function , the bt method can be used to verify a reality of peaks which are computed using a method giving the better resolution ( for example the fft method ) .\\nit is valuable to remember that the power spectrum methods should be applied very carefully .\\nthe difficulties in the interpretation of significant peaks could be caused by at least four effects : a sampling of a continuos function , an echo - effect , a contribution of long - term periodicities and a random noise .\\nfirst effect exists because periodicities , which are shorter than the sampling interval , may mix with longer periodicities . in result , this effect can be reduced by an decrease of the sampling interval between observations .\\nthe echo - effect occurs when there is a latent harmonic of frequency @xmath10 in the time series , giving a spectral peak at @xmath10 , and also periodic terms of frequency @xmath11 etc .\\nthis may be detected by the autocorrelation function for time series with a large variance .\\ntime series often contain long - term periodicities , that influence short - term peaks .\\nthey could rise periodogram s peaks at lower frequencies .\\nhowever , it is also easy to notice the influence of the long - term periodicities on short - term peaks in the graphs of the autocorrelation functions .\\nthis effect is observed for the time series of solar activity indexes which are limited by the 11-year cycle .    to find statistically significant periodicities\\nit is reasonable to use the autocorrelation function and the power spectrum method with a high resolution . in the case of a stationary time\\nseries they give similar results .\\nmoreover , for a stationary time series with the mean zero the fourier transform is equivalent to the cosine transform of an autocorrelation function @xcite .\\nthus , after a comparison of a periodogram with an appropriate autocorrelation function one can detect peaks which are in the graph of the first function and do not exist in the graph of the second function .\\nthe reasons of their existence could be explained by the long - term periodicities and the echo - effect .\\nbelow method enables one to detect these effects .\\n( solid line ) and the 95% confidence level basing on thered noise ( dotted line ) .\\nthe periodogram values are presented on the left axis .\\nthe lower curve illustrates the autocorrelation function of the same time series ( solid line ) .\\nthe dotted lines represent two standard errors of the autocorrelation function .\\nthe dashed horizontal line shows the zero level .\\nthe autocorrelation values are shown in the right axis . ]     because the statistical tests indicate that the time series is a white noise the confidence level is not marked . ]    . ]\\nthe method of the diagnosis of an echo - effect in the power spectrum ( de ) consists in an analysis of a periodogram of a given time series computed using the bt method .\\nthe bt method bases on the cosine transform of the autocorrelation function which creates peaks which are in the periodogram , but not in the autocorrelation function .\\nthe de method is used for peaks which are computed by the fft method ( with high resolution ) and are statistically significant .\\nthe time series of sunspot activity indexes with the spacing interval one rotation or one month contain a markov - type persistence , which means a tendency for the successive values of the time series to remember their antecendent values .\\nthus , i use a confidence level basing on the red noise of markov @xcite for the choice of the significant peaks of the periodogram computed by the fft method .\\nwhen a time series does not contain the markov - type persistence i apply the fisher test and the kolmogorov - smirnov test at the significance level @xmath12 @xcite to verify a statistically significance of periodograms peaks . the fisher test checks the null hypothesis that the time series is white noise agains the alternative hypothesis that the time series contains an added deterministic periodic component of unspecified frequency . because the fisher test tends to be severe in rejecting peaks as insignificant the kolmogorov - smirnov test is also used .\\nthe de method analyses raw estimators of the power spectrum .\\nthey are given as follows    @xmath13    for @xmath14 + where @xmath15 for @xmath16 + @xmath17 is the length of the time series @xmath18 and @xmath19 is the mean value .\\nthe first term of the estimator @xmath20 is constant .\\nthe second term takes two values ( depending on odd or even @xmath21 ) which are not significant because @xmath22 for large m. thus , the third term of ( 1 ) should be analysed .\\nlooking for intervals of @xmath23 for which @xmath24 has the same sign and different signs one can find such parts of the function @xmath25 which create the value @xmath20 .\\nlet the set of values of the independent variable of the autocorrelation function be called @xmath26 and it can be divided into the sums of disjoint sets : @xmath27 where + @xmath28 + @xmath29 @xmath30 @xmath31 + @xmath32 + @xmath33 @xmath34 @xmath35 @xmath36 @xmath37 @xmath38\\n@xmath39 @xmath40    well , the set @xmath41 contains all integer values of @xmath23 from the interval of @xmath42 for which the autocorrelation function and the cosinus function with the period @xmath43 $ ] are positive .\\nthe index @xmath44 indicates successive parts of the cosinus function for which the cosinuses of successive values of @xmath23 have the same sign .\\nhowever , sometimes the set @xmath41 can be empty .\\nfor example , for @xmath45 and @xmath46 the set @xmath47 should contain all @xmath48 $ ] for which @xmath49 and @xmath50 , but for such values of @xmath23 the values of @xmath51 are negative .\\nthus , the set @xmath47 is empty .    .\\nthe periodogram values are presented on the left axis .\\nthe lower curve illustrates the autocorrelation function of the same time series .\\nthe autocorrelation values are shown in the right axis . ]\\nlet us take into consideration all sets \\\\{@xmath52 } , \\\\{@xmath53 } and \\\\{@xmath41 } which are not empty . because numberings and power of these sets depend on the form of the autocorrelation function of the given time series , it is impossible to establish them arbitrary .\\nthus , the sets of appropriate indexes of the sets \\\\{@xmath52 } , \\\\{@xmath53 } and \\\\{@xmath41 } are called @xmath54 , @xmath55 and @xmath56 respectively . for example\\nthe set @xmath56 contains all @xmath44 from the set @xmath57 for which the sets @xmath41 are not empty .\\nto separate quantitatively in the estimator @xmath20 the positive contributions which are originated by the cases described by the formula ( 5 ) from the cases which are described by the formula ( 3 ) the following indexes are introduced : @xmath58 @xmath59 @xmath60 @xmath61 where @xmath62 @xmath63 @xmath64 taking for the empty sets \\\\{@xmath53 } and \\\\{@xmath41 } the indices @xmath65 and @xmath66 equal zero .\\nthe index @xmath65 describes a percentage of the contribution of the case when @xmath25 and @xmath51 are positive to the positive part of the third term of the sum ( 1 ) .\\nthe index @xmath66 describes a similar contribution , but for the case when the both @xmath25 and @xmath51 are simultaneously negative .\\nthanks to these one can decide which the positive or the negative values of the autocorrelation function have a larger contribution to the positive values of the estimator @xmath20 .\\nwhen the difference @xmath67 is positive , the statement the @xmath21-th peak really exists can not be rejected .\\nthus , the following formula should be satisfied : @xmath68    because the @xmath21-th peak could exist as a result of the echo - effect , it is necessary to verify the second condition :    @xmath69\\\\in c_m.\\\\ ] ]    .\\nthe periodogram values are presented on the left axis .\\nthe lower curve illustrates the autocorrelation function of the same time series ( solid line ) .\\nthe dotted lines represent two standard errors of the autocorrelation function .\\nthe dashed horizontal line shows the zero level .\\nthe autocorrelation values are shown in the right axis . ]    to verify the implication ( 8) firstly it is necessary to evaluate the sets @xmath41 for @xmath70 of the values of @xmath23 for which the autocorrelation function and the cosine function with the period @xmath71 $ ] are positive and the sets @xmath72 of values of @xmath23 for which the autocorrelation function and the cosine function with the period @xmath43 $ ] are negative .\\nsecondly , a percentage of the contribution of the sum of products of positive values of @xmath25 and @xmath51 to the sum of positive products of the values of @xmath25 and @xmath51 should be evaluated . as a result the indexes @xmath65 for each set\\n@xmath41 where @xmath44 is the index from the set @xmath56 are obtained .\\nthirdly , from all sets @xmath41 such that @xmath70 the set @xmath73 for which the index @xmath65 is the greatest should be chosen .    the implication ( 8) is true when the set @xmath73 includes the considered period @xmath43 $ ] .\\nthis means that the greatest contribution of positive values of the autocorrelation function and positive cosines with the period @xmath43 $ ] to the periodogram value @xmath20 is caused by the sum of positive products of @xmath74 for each @xmath75-\\\\frac{m}{2k},[\\\\frac{2m}{k}]+\\\\frac{m}{2k})$ ] .    when the implication ( 8) is false , the peak @xmath20 is mainly created by the sum of positive products of @xmath74 for each @xmath76-\\\\frac{m}{2k},\\\\big [ \\\\frac{2m}{n}\\\\big ] + \\\\frac{m}{2k } \\\\big ) $ ] , where @xmath77 is a multiple or a divisor of @xmath21 .\\nit is necessary to add , that the de method should be applied to the periodograms peaks , which probably exist because of the echo - effect .\\nit enables one to find such parts of the autocorrelation function , which have the significant contribution to the considered peak .\\nthe fact , that the conditions ( 7 ) and ( 8) are satisfied , can unambiguously decide about the existence of the considered periodicity in the given time series , but if at least one of them is not satisfied , one can doubt about the existence of the considered periodicity .\\nthus , in such cases the sentence the peak can not be treated as true should be used .    using the de method\\nit is necessary to remember about the power of the set @xmath78 .\\nif @xmath79 is too large , errors of an autocorrelation function estimation appear .\\nthey are caused by the finite length of the given time series and as a result additional peaks of the periodogram occur . if @xmath79 is too small , there are less peaks because of a low resolution of the periodogram . in applications\\n@xmath80 is used . in order to evaluate the value\\n@xmath79 the fft method is used .\\nthe periodograms computed by the bt and the fft method are compared .\\nthe conformity of them enables one to obtain the value @xmath79 .    .\\nthe fft periodogram values are presented on the left axis .\\nthe lower curve illustrates the bt periodogram of the same time series ( solid line and large black circles ) .\\nthe bt periodogram values are shown in the right axis . ]\\nin this paper the sunspot activity data ( august 1923 - october 1933 ) provided by the greenwich photoheliographic results ( gpr ) are analysed .\\nfirstly , i consider the monthly sunspot number data . to eliminate the 11-year trend from these data ,\\nthe consecutively smoothed monthly sunspot number @xmath81 is subtracted from the monthly sunspot number @xmath82 where the consecutive mean @xmath83 is given by @xmath84 the values @xmath83 for @xmath85 and @xmath86 are calculated using additional data from last six months of cycle 15 and first six months of cycle 17 .    because of the north - south asymmetry of various solar indices @xcite , the sunspot activity is considered for each solar hemisphere separately .\\nanalogously to the monthly sunspot numbers , the time series of sunspot areas in the northern and southern hemispheres with the spacing interval @xmath87 rotation are denoted . in order to find periodicities ,\\nthe following time series are used : + @xmath88  \\n+ @xmath89\\n   + @xmath90     + in the lower part of figure [ f1 ] the autocorrelation function of the time series for the northern hemisphere @xmath88 is shown .\\nit is easy to notice that the prominent peak falls at 17 rotations interval ( 459 days ) and @xmath25 for @xmath91 $ ] rotations ( [ 81 , 162 ] days ) are significantly negative .\\nthe periodogram of the time series @xmath88 ( see the upper curve in figures [ f1 ] ) does not show the significant peaks at @xmath92 rotations ( 135 , 162 days ) , but there is the significant peak at @xmath93 ( 243 days ) .\\nthe peaks at @xmath94 are close to the peaks of the autocorrelation function .\\nthus , the result obtained for the periodicity at about @xmath0 days are contradict to the results obtained for the time series of daily sunspot areas @xcite .    for the southern hemisphere ( the lower curve in figure [ f2 ] ) @xmath25 for @xmath95 $ ] rotations ( [ 54 , 189 ] days ) is not positive except @xmath96 ( 135 days ) for which @xmath97 is not statistically significant .\\nthe upper curve in figures [ f2 ] presents the periodogram of the time series @xmath89 .\\nthis time series does not contain a markov - type persistence .\\nmoreover , the kolmogorov - smirnov test and the fisher test do not reject a null hypothesis that the time series is a white noise only .\\nthis means that the time series do not contain an added deterministic periodic component of unspecified frequency .\\nthe autocorrelation function of the time series @xmath90 ( the lower curve in figure [ f3 ] ) has only one statistically significant peak for @xmath98 months ( 480 days ) and negative values for @xmath99 $ ] months ( [ 90 , 390 ] days ) .\\nhowever , the periodogram of this time series ( the upper curve in figure [ f3 ] ) has two significant peaks the first at 15.2 and the second at 5.3 months ( 456 , 159 days ) .\\nthus , the periodogram contains the significant peak , although the autocorrelation function has the negative value at @xmath100 months .    to explain\\nthese problems two following time series of daily sunspot areas are considered : + @xmath101  \\n+ @xmath102     + where @xmath103    the values @xmath104 for @xmath105 and @xmath106 are calculated using additional daily data from the solar cycles 15 and 17 .     and the cosine function for @xmath45 ( the period at about 154 days ) .\\nthe horizontal line ( dotted line ) shows the zero level .\\nthe vertical dotted lines evaluate the intervals where the sets @xmath107 ( for @xmath108 ) are searched .\\nthe percentage values show the index @xmath65 for each @xmath41 for the time series @xmath102 ( in parentheses for the time series @xmath101 ) . in the right bottom corner\\nthe values of @xmath65 for the time series @xmath102 , for @xmath109 are written . ]\\n( the 500-day period ) ]    the comparison of the functions @xmath25 of the time series @xmath101 ( the lower curve in figure [ f4 ] ) and @xmath102 ( the lower curve in figure [ f5 ] ) suggests that the positive values of the function @xmath110 of the time series @xmath101 in the interval of @xmath111 $ ] days could be caused by the 11-year cycle .\\nthis effect is not visible in the case of periodograms of the both time series computed using the fft method ( see the upper curves in figures [ f4 ] and [ f5 ] ) or the bt method ( see the lower curve in figure [ f6 ] ) . moreover , the periodogram of the time series @xmath102 has the significant values at @xmath112 days , but the autocorrelation function is negative at these points .\\n@xcite showed that the lomb - scargle periodograms for the both time series ( see @xcite , figures 7 a - c ) have a peak at 158.8 days which stands over the fap level by a significant amount . using the de method the above discrepancies are obvious . to establish the @xmath79 value the periodograms computed by the fft and\\nthe bt methods are shown in figure [ f6 ] ( the upper and the lower curve respectively ) .\\nfor @xmath46 and for periods less than 166 days there is a good comformity of the both periodograms ( but for periods greater than 166 days the points of the bt periodogram are not linked because the bt periodogram has much worse resolution than the fft periodogram ( no one know how to do it ) ) . for @xmath46 and @xmath113\\nthe value of @xmath21 is 13 ( @xmath71=153 $ ] ) .\\nthe inequality ( 7 ) is satisfied because @xmath114 .\\nthis means that the value of @xmath115 is mainly created by positive values of the autocorrelation function .\\nthe implication ( 8) needs an evaluation of the greatest value of the index @xmath65 where @xmath70 , but the solar data contain the most prominent period for @xmath116 days because of the solar rotation .\\nthus , although @xmath117 for each @xmath118 , all sets @xmath41 ( see ( 5 ) and ( 6 ) ) without the set @xmath119 ( see ( 4 ) ) , which contains @xmath120 $ ] , are considered . this situation is presented in figure [ f7 ] . in this figure\\ntwo curves @xmath121 and @xmath122 are plotted .\\nthe vertical dotted lines evaluate the intervals where the sets @xmath107 ( for @xmath123 ) are searched . for such @xmath41 two numbers\\nare written : in parentheses the value of @xmath65 for the time series @xmath101 and above it the value of @xmath65 for the time series @xmath102 . to make this figure clear the curves are plotted for the set @xmath124 only .\\n( in the right bottom corner information about the values of @xmath65 for the time series @xmath102 , for @xmath109 are written . )\\nthe implication ( 8) is not true , because @xmath125 for @xmath126 .\\ntherefore , @xmath43=153\\\\notin c_6=[423,500]$ ] .\\nmoreover , the autocorrelation function for @xmath127 $ ] is negative and the set @xmath128 is empty .\\nthus , @xmath129 . on the basis of these information one can state , that the periodogram peak at @xmath130 days of the time series @xmath102 exists because of positive @xmath25 , but for @xmath23 from the intervals which do not contain this period .\\nlooking at the values of @xmath65 of the time series @xmath101 , one can notice that they decrease when @xmath23 increases until @xmath131 .\\nthis indicates , that when @xmath23 increases , the contribution of the 11-year cycle to the peaks of the periodogram decreases .\\nan increase of the value of @xmath65 is for @xmath132 for the both time series , although the contribution of the 11-year cycle for the time series @xmath101 is insignificant .\\nthus , this part of the autocorrelation function ( @xmath133 for the time series @xmath102 ) influences the @xmath21-th peak of the periodogram .\\nthis suggests that the periodicity at about 155 days is a harmonic of the periodicity from the interval of @xmath1 $ ] days .\\n( solid line ) and consecutively smoothed sunspot areas of the one rotation time interval @xmath134 ( dotted line ) .\\nboth indexes are presented on the left axis .\\nthe lower curve illustrates fluctuations of the sunspot areas @xmath135 .\\nthe dotted and dashed horizontal lines represent levels zero and @xmath136 respectively .\\nthe fluctuations are shown on the right axis . ]\\nthe described reasoning can be carried out for other values of the periodogram .\\nfor example , the condition ( 8) is not satisfied for @xmath137 ( 250 , 222 , 200 days ) .\\nmoreover , the autocorrelation function at these points is negative .\\nthese suggest that there are not a true periodicity in the interval of [ 200 , 250 ] days .\\nit is difficult to decide about the existence of the periodicities for @xmath138 ( 333 days ) and @xmath139 ( 286 days ) on the basis of above analysis . the implication ( 8) is not satisfied for @xmath139 and the condition ( 7 ) is not satisfied for @xmath138 , although the function @xmath25 of the time series @xmath102 is significantly positive for @xmath140 .\\nthe conditions ( 7 ) and ( 8) are satisfied for @xmath141 ( figure [ f8 ] ) and @xmath142 . therefore , it is possible to exist the periodicity from the interval of @xmath1 $ ] days .\\nsimilar results were also obtained by @xcite for daily sunspot numbers and daily sunspot areas .\\nshe considered the means of three periodograms of these indexes for data from @xmath143 years and found statistically significant peaks from the interval of @xmath1 $ ] ( see @xcite , figure 2 ) .\\n@xcite studied sunspot areas from 1876 - 1999 and sunspot numbers from 1749 - 2001 with the help of the wavelet transform .\\nthey pointed out that the 154 - 158-day period could be the third harmonic of the 1.3-year ( 475-day ) period .\\nmoreover , the both periods fluctuate considerably with time , being stronger during stronger sunspot cycles .\\ntherefore , the wavelet analysis suggests a common origin of the both periodicities . this conclusion confirms the de method result which indicates that the periodogram peak at @xmath144 days is an alias of the periodicity from the interval of @xmath1 $ ]\\nin order to verify the existence of the periodicity at about 155 days i consider the following time series : + @xmath145     + @xmath146\\n   + @xmath147  \\n+ the value @xmath134 is calculated analogously to @xmath83 ( see sect .\\nthe values @xmath148 and @xmath149 are evaluated from the formula ( 9 ) . in the upper part of figure [ f9 ] the time series of sunspot areas @xmath150 of the one rotation time interval from the whole solar disk and the time series of consecutively smoothed sunspot areas @xmath151\\nare showed . in the lower part of figure [ f9 ]\\nthe time series of sunspot area fluctuations @xmath145 is presented .\\non the basis of these data the maximum activity period of cycle 16 is evaluated .\\nit is an interval between two strongest fluctuations e.a .\\n@xmath152 $ ] rotations .\\nthe length of the time interval @xmath153 is 54 rotations .\\nif the about @xmath0-day ( 6 solar rotations ) periodicity existed in this time interval and it was characteristic for strong fluctuations from this time interval , 10 local maxima in the set of @xmath154 would be seen .\\nthen it should be necessary to find such a value of p for which @xmath155 for @xmath156 and the number of the local maxima of these values is 10 .\\nas it can be seen in the lower part of figure [ f9 ] this is for the case of @xmath157 ( in this figure the dashed horizontal line is the level of @xmath158 ) .\\nfigure [ f10 ] presents nine time distances among the successive fluctuation local maxima and the horizontal line represents the 6-rotation periodicity .\\nit is immediately apparent that the dispersion of these points is 10 and it is difficult to find even few points which oscillate around the value of 6 .\\nsuch an analysis was carried out for smaller and larger @xmath136 and the results were similar .\\ntherefore , the fact , that the about @xmath0-day periodicity exists in the time series of sunspot area fluctuations during the maximum activity period is questionable .    .\\nthe horizontal line represents the 6-rotation ( 162-day ) period . ]    ]    ]    to verify again the existence of the about @xmath0-day periodicity during the maximum activity period in each solar hemisphere separately , the time series @xmath88 and @xmath89 were also cut down to the maximum activity period ( january 1925december 1930 ) .\\nthe comparison of the autocorrelation functions of these time series with the appriopriate autocorrelation functions of the time series @xmath88 and @xmath89 , which are computed for the whole 11-year cycle ( the lower curves of figures [ f1 ] and [ f2 ] ) , indicates that there are not significant differences between them especially for @xmath23=5 and 6 rotations ( 135 and 162 days ) ) .\\nthis conclusion is confirmed by the analysis of the time series @xmath146 for the maximum activity period .\\nthe autocorrelation function ( the lower curve of figure [ f11 ] ) is negative for the interval of [ 57 , 173 ] days , but the resolution of the periodogram is too low to find the significant peak at @xmath159 days .\\nthe autocorrelation function gives the same result as for daily sunspot area fluctuations from the whole solar disk ( @xmath160 ) ( see also the lower curve of figures [ f5 ] ) . in the case of\\nthe time series @xmath89 @xmath161 is zero for the fluctuations from the whole solar cycle and it is almost zero ( @xmath162 ) for the fluctuations from the maximum activity period .\\nthe value @xmath163 is negative .\\nsimilarly to the case of the northern hemisphere the autocorrelation function and the periodogram of southern hemisphere daily sunspot area fluctuations from the maximum activity period @xmath147 are computed ( see figure [ f12 ] ) .\\nthe autocorrelation function has the statistically significant positive peak in the interval of [ 155 , 165 ] days , but the periodogram has too low resolution to decide about the possible periodicities .\\nthe correlative analysis indicates that there are positive fluctuations with time distances about @xmath0 days in the maximum activity period .\\nthe results of the analyses of the time series of sunspot area fluctuations from the maximum activity period are contradict with the conclusions of @xcite .\\nshe uses the power spectrum analysis only .\\nthe periodogram of daily sunspot fluctuations contains peaks , which could be harmonics or subharmonics of the true periodicities .\\nthey could be treated as real periodicities .\\nthis effect is not visible for sunspot data of the one rotation time interval , but averaging could lose true periodicities .\\nthis is observed for data from the southern hemisphere .\\nthere is the about @xmath0-day peak in the autocorrelation function of daily fluctuations , but the correlation for data of the one rotation interval is almost zero or negative at the points @xmath164 and 6 rotations .\\nthus , it is reasonable to research both time series together using the correlative and the power spectrum analyses .\\nthe following results are obtained :    1 .\\na new method of the detection of statistically significant peaks of the periodograms enables one to identify aliases in the periodogram .\\n2 .   two effects cause the existence of the peak of the periodogram of the time series of sunspot area fluctuations at about @xmath0 days : the first is caused by the 27-day periodicity , which probably creates the 162-day periodicity ( it is a subharmonic frequency of the 27-day periodicity ) and the second is caused by statistically significant positive values of the autocorrelation function from the intervals of @xmath165 $ ] and @xmath166 $ ] days .\\nthe existence of the periodicity of about @xmath0 days of the time series of sunspot area fluctuations and sunspot area fluctuations from the northern hemisphere during the maximum activity period is questionable .\\nthe autocorrelation analysis of the time series of sunspot area fluctuations from the southern hemisphere indicates that the periodicity of about 155 days exists during the maximum activity period .\\ni appreciate valuable comments from professor j. jakimiec . the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \\n the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \\n a new method of the diagnosis of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \\n it proves against the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \\n however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . introduction\\nmethods of periodicity analysis\\na method of the diagnosis of an echo-effect in the power spectrum\\ndata analysis\\nthe periodicity at about 155 days during the maximum activity period\\nconclusion\\nacknowledgments'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textt=' '\n",
    "for x in test_token:\n",
    "    textt += test_token[x]\n",
    "textt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8330d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9028542a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for about 20 years the problem of properties of short - term changes of solar activity has been considered extensively .many investigators studied the short - term periodicities of the various indices of solar activity .several periodicities were detected , but the periodicities about 155 days and from the interval of 3  ] days ( 4  ] years ) are mentioned most often .first of them was discovered by  in the occurence rate of gamma - ray flares detected by the gamma - ray spectrometer aboard the _ solar maximum mission ( smm ) .this periodicity was confirmed for other solar flares data and for the same time period  .it was also found in proton flares during solar cycles 19 and 20  , but it was not found in the solar flares data during solar cycles 22  ._    several autors confirmed above results for the daily sunspot area data .  studied the sunspot data from 18741984 .she found the 155-day periodicity in data records from 31 years .this periodicity is always characteristic for one of the solar hemispheres ( the southern hemisphere for cycles 1215 and the northern hemisphere for cycles 1621 ) .moreover , it is only present during epochs of maximum activity ( in episodes of 13 years ) .similarinvestigationswerecarriedoutby +  .they applied the same power spectrum method as lean , but the daily sunspot area data ( cycles 1221 ) were divided into 10 shorter time series .the periodicities were searched for the frequency interval 57115 nhz ( 100200 days ) and for each of 10 time series .the authors showed that the periodicity between 150160 days is statistically significant during all cycles from 16 to 21 .the considered peaks were remained unaltered after removing the 11-year cycle and applying the power spectrum analysis . used the wavelet technique for the daily sunspot areas between 1874 and 1993 .they determined the epochs of appearance of this periodicity and concluded that it presents around the maximum activity period in cycles 16 to 21 .moreover , the power of this periodicity started growing at cycle 19 , decreased in cycles 20 and 21 and disappered after cycle 21 .similaranalyseswerepresentedby +  , but for sunspot number , solar wind plasma , interplanetary magnetic field and geomagnetic activity index 5 .during 1964 - 2000 the sunspot number wavelet power of periods less than one year shows a cyclic evolution with the phase of the solar cycle.the 154-day period is prominent and its strenth is stronger around the 1982 - 1984 interval in almost all solar wind parameters .the existence of the 156-day periodicity in sunspot data were confirmed by  .they considered the possible relation between the 475-day ( 1.3-year ) and 156-day periodicities .the 475-day ( 1.3-year ) periodicity was also detected in variations of the interplanetary magnetic field , geomagnetic activity helioseismic data and in the solar wind speed  . concluded that the region of larger wavelet power shifts from 475-day ( 1.3-year ) period to 620-day ( 1.7-year ) period and then back to 475-day ( 1.3-year ) .the periodicities from the interval 6  ] days ( 4  ] years ) have been considered from 1968 . mentioned a 16.3-month ( 490-day ) periodicity in the sunspot numbers and in the geomagnetic data . analysed the occurrence rate of major flares during solar cycles 19 .they found a 18-month ( 540-day ) periodicity in flare rate of the norhern hemisphere . confirmed this result for the 7 flare data for solar cycles 20 and 21 and found a peak in the power spectra near 510540 days . found a 17-month ( 510-day ) periodicity of sunspot groups and their areas from 1969 to 1986 .these authors concluded that the length of this period is variable and the reason of this periodicity is still not understood . and +  obtained statistically significant peaks of power at around 158 days for daily sunspot data from 1923 - 1933 ( cycle 16 ) . in this paper the problem of the existence of this periodicity for sunspot data from cycle 16 is considered .the daily sunspot areas , the mean sunspot areas per carrington rotation , the monthly sunspot numbers and their fluctuations , which are obtained after removing the 11-year cycle are analysed . in section 2 the properties of the power spectrum methods are described . in section 3 a new approach to the problem of aliases in the power spectrum analysisis presented . in section 4 numerical results of the new method of the diagnosis of an echo - effect for sunspot area data are discussed . in section 5 the problem of the existence of the periodicity of about 155 days during the maximum activity period for sunspot data from the whole solar disk and from each solar hemisphere separately is considered .to find periodicities in a given time series the power spectrum analysis is applied . in this papertwo methods are used : the fast fourier transformation algorithm with the hamming window function ( fft ) and the blackman - tukey ( bt ) power spectrum method  .the bt method is used for the diagnosis of the reasons of the existence of peaks , which are obtained by the fft method .the bt method consists in the smoothing of a cosine transform of an autocorrelation function using a 3-point weighting average .such an estimator is consistent and unbiased .moreover , the peaks are uncorrelated and their sum is a variance of a considered time series . the main disadvantage of this method is a weak resolution of the periodogram points , particularly for low frequences .for example , if the autocorrelation function is evaluated for 8 , then the distribution points in the time domain are : 9 thus , it is obvious that this method should not be used for detecting low frequency periodicities with a fairly good resolution .however , because of an application of the autocorrelation function , the bt method can be used to verify a reality of peaks which are computed using a method giving the better resolution ( for example the fft method ) .it is valuable to remember that the power spectrum methods should be applied very carefully .the difficulties in the interpretation of significant peaks could be caused by at least four effects : a sampling of a continuos function , an echo - effect , a contribution of long - term periodicities and a random noise .first effect exists because periodicities , which are shorter than the sampling interval , may mix with longer periodicities . in result , this effect can be reduced by an decrease of the sampling interval between observations .the echo - effect occurs when there is a latent harmonic of frequency 10 in the time series , giving a spectral peak at 10 , and also periodic terms of frequency 11 etc .this may be detected by the autocorrelation function for time series with a large variance .time series often contain long - term periodicities , that influence short - term peaks .they could rise periodogram s peaks at lower frequencies .however , it is also easy to notice the influence of the long - term periodicities on short - term peaks in the graphs of the autocorrelation functions .this effect is observed for the time series of solar activity indexes which are limited by the 11-year cycle .    to find statistically significant periodicitiesit is reasonable to use the autocorrelation function and the power spectrum method with a high resolution . in the case of a stationary timeseries they give similar results .moreover , for a stationary time series with the mean zero the fourier transform is equivalent to the cosine transform of an autocorrelation function  .thus , after a comparison of a periodogram with an appropriate autocorrelation function one can detect peaks which are in the graph of the first function and do not exist in the graph of the second function .the reasons of their existence could be explained by the long - term periodicities and the echo - effect .below method enables one to detect these effects .( solid line ) and the 95% confidence level basing on thered noise ( dotted line ) .the periodogram values are presented on the left axis .the lower curve illustrates the autocorrelation function of the same time series ( solid line ) .the dotted lines represent two standard errors of the autocorrelation function .the dashed horizontal line shows the zero level .the autocorrelation values are shown in the right axis . ]     because the statistical tests indicate that the time series is a white noise the confidence level is not marked . ]    . ]the method of the diagnosis of an echo - effect in the power spectrum ( de ) consists in an analysis of a periodogram of a given time series computed using the bt method .the bt method bases on the cosine transform of the autocorrelation function which creates peaks which are in the periodogram , but not in the autocorrelation function .the de method is used for peaks which are computed by the fft method ( with high resolution ) and are statistically significant .the time series of sunspot activity indexes with the spacing interval one rotation or one month contain a markov - type persistence , which means a tendency for the successive values of the time series to remember their antecendent values .thus , i use a confidence level basing on the red noise of markov  for the choice of the significant peaks of the periodogram computed by the fft method .when a time series does not contain the markov - type persistence i apply the fisher test and the kolmogorov - smirnov test at the significance level 12  to verify a statistically significance of periodograms peaks . the fisher test checks the null hypothesis that the time series is white noise agains the alternative hypothesis that the time series contains an added deterministic periodic component of unspecified frequency . because the fisher test tends to be severe in rejecting peaks as insignificant the kolmogorov - smirnov test is also used .the de method analyses raw estimators of the power spectrum .they are given as follows    13    for 14 + where 15 for 16 + 17 is the length of the time series 18 and 19 is the mean value .the first term of the estimator 20 is constant .the second term takes two values ( depending on odd or even 21 ) which are not significant because 22 for large m. thus , the third term of ( 1 ) should be analysed .looking for intervals of 23 for which 24 has the same sign and different signs one can find such parts of the function 25 which create the value 20 .let the set of values of the independent variable of the autocorrelation function be called 26 and it can be divided into the sums of disjoint sets : 27 where + 28 + 29 30 31 + 32 + 33 34 35 36 37 3839 40    well , the set 41 contains all integer values of 23 from the interval of 42 for which the autocorrelation function and the cosinus function with the period 43  ] are positive .the index 44 indicates successive parts of the cosinus function for which the cosinuses of successive values of 23 have the same sign .however , sometimes the set 41 can be empty .for example , for 45 and 46 the set 47 should contain all 48  ] for which 49 and 50 , but for such values of 23 the values of 51 are negative .thus , the set 47 is empty .    .the periodogram values are presented on the left axis .the lower curve illustrates the autocorrelation function of the same time series .the autocorrelation values are shown in the right axis . ]let us take into consideration all sets \\\\{52 } , \\\\{53 } and \\\\{41 } which are not empty . because numberings and power of these sets depend on the form of the autocorrelation function of the given time series , it is impossible to establish them arbitrary .thus , the sets of appropriate indexes of the sets \\\\{52 } , \\\\{53 } and \\\\{41 } are called 54 , 55 and 56 respectively . for examplethe set 56 contains all 44 from the set 57 for which the sets 41 are not empty .to separate quantitatively in the estimator 20 the positive contributions which are originated by the cases described by the formula ( 5 ) from the cases which are described by the formula ( 3 ) the following indexes are introduced : 58 59 60 61 where 62 63 64 taking for the empty sets \\\\{53 } and \\\\{41 } the indices 65 and 66 equal zero .the index 65 describes a percentage of the contribution of the case when 25 and 51 are positive to the positive part of the third term of the sum ( 1 ) .the index 66 describes a similar contribution , but for the case when the both 25 and 51 are simultaneously negative .thanks to these one can decide which the positive or the negative values of the autocorrelation function have a larger contribution to the positive values of the estimator 20 .when the difference 67 is positive , the statement the 21-th peak really exists can not be rejected .thus , the following formula should be satisfied : 68    because the 21-th peak could exist as a result of the echo - effect , it is necessary to verify the second condition :    69\\\\in c_m.\\\\ ] ]    .the periodogram values are presented on the left axis .the lower curve illustrates the autocorrelation function of the same time series ( solid line ) .the dotted lines represent two standard errors of the autocorrelation function .the dashed horizontal line shows the zero level .the autocorrelation values are shown in the right axis . ]    to verify the implication ( 8) firstly it is necessary to evaluate the sets 41 for 70 of the values of 23 for which the autocorrelation function and the cosine function with the period 71  ] are positive and the sets 72 of values of 23 for which the autocorrelation function and the cosine function with the period 43  ] are negative .secondly , a percentage of the contribution of the sum of products of positive values of 25 and 51 to the sum of positive products of the values of 25 and 51 should be evaluated . as a result the indexes 65 for each set41 where 44 is the index from the set 56 are obtained .thirdly , from all sets 41 such that 70 the set 73 for which the index 65 is the greatest should be chosen .    the implication ( 8) is true when the set 73 includes the considered period 43  ] .this means that the greatest contribution of positive values of the autocorrelation function and positive cosines with the period 43  ] to the periodogram value 20 is caused by the sum of positive products of 74 for each 75-\\\\frac{m}{2k},[\\\\frac{2m}{k}]+\\\\frac{m}{2k}) ] .    when the implication ( 8) is false , the peak 20 is mainly created by the sum of positive products of 74 for each 76-\\\\frac{m}{2k},\\\\big [ \\\\frac{2m}{n}\\\\big ] + \\\\frac{m}{2k } \\\\big )  ] , where 77 is a multiple or a divisor of 21 .it is necessary to add , that the de method should be applied to the periodograms peaks , which probably exist because of the echo - effect .it enables one to find such parts of the autocorrelation function , which have the significant contribution to the considered peak .the fact , that the conditions ( 7 ) and ( 8) are satisfied , can unambiguously decide about the existence of the considered periodicity in the given time series , but if at least one of them is not satisfied , one can doubt about the existence of the considered periodicity .thus , in such cases the sentence the peak can not be treated as true should be used .    using the de methodit is necessary to remember about the power of the set 78 .if 79 is too large , errors of an autocorrelation function estimation appear .they are caused by the finite length of the given time series and as a result additional peaks of the periodogram occur . if 79 is too small , there are less peaks because of a low resolution of the periodogram . in applications80 is used . in order to evaluate the value79 the fft method is used .the periodograms computed by the bt and the fft method are compared .the conformity of them enables one to obtain the value 79 .    .the fft periodogram values are presented on the left axis .the lower curve illustrates the bt periodogram of the same time series ( solid line and large black circles ) .the bt periodogram values are shown in the right axis . ]in this paper the sunspot activity data ( august 1923 - october 1933 ) provided by the greenwich photoheliographic results ( gpr ) are analysed .firstly , i consider the monthly sunspot number data . to eliminate the 11-year trend from these data ,the consecutively smoothed monthly sunspot number 81 is subtracted from the monthly sunspot number 82 where the consecutive mean 83 is given by 84 the values 83 for 85 and 86 are calculated using additional data from last six months of cycle 15 and first six months of cycle 17 .    because of the north - south asymmetry of various solar indices  , the sunspot activity is considered for each solar hemisphere separately .analogously to the monthly sunspot numbers , the time series of sunspot areas in the northern and southern hemispheres with the spacing interval 87 rotation are denoted . in order to find periodicities ,the following time series are used : + 88  + 89   + 90     + in the lower part of figure [ f1 ] the autocorrelation function of the time series for the northern hemisphere 88 is shown .it is easy to notice that the prominent peak falls at 17 rotations interval ( 459 days ) and 25 for 91  ] rotations ( [ 81 , 162 ] days ) are significantly negative .the periodogram of the time series 88 ( see the upper curve in figures [ f1 ] ) does not show the significant peaks at 92 rotations ( 135 , 162 days ) , but there is the significant peak at 93 ( 243 days ) .the peaks at 94 are close to the peaks of the autocorrelation function .thus , the result obtained for the periodicity at about 0 days are contradict to the results obtained for the time series of daily sunspot areas  .    for the southern hemisphere ( the lower curve in figure [ f2 ] ) 25 for 95  ] rotations ( [ 54 , 189 ] days ) is not positive except 96 ( 135 days ) for which 97 is not statistically significant .the upper curve in figures [ f2 ] presents the periodogram of the time series 89 .this time series does not contain a markov - type persistence .moreover , the kolmogorov - smirnov test and the fisher test do not reject a null hypothesis that the time series is a white noise only .this means that the time series do not contain an added deterministic periodic component of unspecified frequency .the autocorrelation function of the time series 90 ( the lower curve in figure [ f3 ] ) has only one statistically significant peak for 98 months ( 480 days ) and negative values for 99  ] months ( [ 90 , 390 ] days ) .however , the periodogram of this time series ( the upper curve in figure [ f3 ] ) has two significant peaks the first at 15.2 and the second at 5.3 months ( 456 , 159 days ) .thus , the periodogram contains the significant peak , although the autocorrelation function has the negative value at 100 months .    to explainthese problems two following time series of daily sunspot areas are considered : + 101  + 102     + where 103    the values 104 for 105 and 106 are calculated using additional daily data from the solar cycles 15 and 17 .     and the cosine function for 45 ( the period at about 154 days ) .the horizontal line ( dotted line ) shows the zero level .the vertical dotted lines evaluate the intervals where the sets 107 ( for 108 ) are searched .the percentage values show the index 65 for each 41 for the time series 102 ( in parentheses for the time series 101 ) . in the right bottom cornerthe values of 65 for the time series 102 , for 109 are written . ]( the 500-day period ) ]    the comparison of the functions 25 of the time series 101 ( the lower curve in figure [ f4 ] ) and 102 ( the lower curve in figure [ f5 ] ) suggests that the positive values of the function 110 of the time series 101 in the interval of 111  ] days could be caused by the 11-year cycle .this effect is not visible in the case of periodograms of the both time series computed using the fft method ( see the upper curves in figures [ f4 ] and [ f5 ] ) or the bt method ( see the lower curve in figure [ f6 ] ) . moreover , the periodogram of the time series 102 has the significant values at 112 days , but the autocorrelation function is negative at these points . showed that the lomb - scargle periodograms for the both time series ( see  , figures 7 a - c ) have a peak at 158.8 days which stands over the fap level by a significant amount . using the de method the above discrepancies are obvious . to establish the 79 value the periodograms computed by the fft andthe bt methods are shown in figure [ f6 ] ( the upper and the lower curve respectively ) .for 46 and for periods less than 166 days there is a good comformity of the both periodograms ( but for periods greater than 166 days the points of the bt periodogram are not linked because the bt periodogram has much worse resolution than the fft periodogram ( no one know how to do it ) ) . for 46 and 113the value of 21 is 13 ( 71=153  ] ) .the inequality ( 7 ) is satisfied because 114 .this means that the value of 115 is mainly created by positive values of the autocorrelation function .the implication ( 8) needs an evaluation of the greatest value of the index 65 where 70 , but the solar data contain the most prominent period for 116 days because of the solar rotation .thus , although 117 for each 118 , all sets 41 ( see ( 5 ) and ( 6 ) ) without the set 119 ( see ( 4 ) ) , which contains 120  ] , are considered . this situation is presented in figure [ f7 ] . in this figuretwo curves 121 and 122 are plotted .the vertical dotted lines evaluate the intervals where the sets 107 ( for 123 ) are searched . for such 41 two numbersare written : in parentheses the value of 65 for the time series 101 and above it the value of 65 for the time series 102 . to make this figure clear the curves are plotted for the set 124 only .( in the right bottom corner information about the values of 65 for the time series 102 , for 109 are written . )the implication ( 8) is not true , because 125 for 126 .therefore , 43=153\\\\notin c_6=[423,500] ] .moreover , the autocorrelation function for 127  ] is negative and the set 128 is empty .thus , 129 . on the basis of these information one can state , that the periodogram peak at 130 days of the time series 102 exists because of positive 25 , but for 23 from the intervals which do not contain this period .looking at the values of 65 of the time series 101 , one can notice that they decrease when 23 increases until 131 .this indicates , that when 23 increases , the contribution of the 11-year cycle to the peaks of the periodogram decreases .an increase of the value of 65 is for 132 for the both time series , although the contribution of the 11-year cycle for the time series 101 is insignificant .thus , this part of the autocorrelation function ( 133 for the time series 102 ) influences the 21-th peak of the periodogram .this suggests that the periodicity at about 155 days is a harmonic of the periodicity from the interval of 1  ] days .( solid line ) and consecutively smoothed sunspot areas of the one rotation time interval 134 ( dotted line ) .both indexes are presented on the left axis .the lower curve illustrates fluctuations of the sunspot areas 135 .the dotted and dashed horizontal lines represent levels zero and 136 respectively .the fluctuations are shown on the right axis . ]the described reasoning can be carried out for other values of the periodogram .for example , the condition ( 8) is not satisfied for 137 ( 250 , 222 , 200 days ) .moreover , the autocorrelation function at these points is negative .these suggest that there are not a true periodicity in the interval of [ 200 , 250 ] days .it is difficult to decide about the existence of the periodicities for 138 ( 333 days ) and 139 ( 286 days ) on the basis of above analysis . the implication ( 8) is not satisfied for 139 and the condition ( 7 ) is not satisfied for 138 , although the function 25 of the time series 102 is significantly positive for 140 .the conditions ( 7 ) and ( 8) are satisfied for 141 ( figure [ f8 ] ) and 142 . therefore , it is possible to exist the periodicity from the interval of 1  ] days .similar results were also obtained by  for daily sunspot numbers and daily sunspot areas .she considered the means of three periodograms of these indexes for data from 143 years and found statistically significant peaks from the interval of 1  ] ( see  , figure 2 ) . studied sunspot areas from 1876 - 1999 and sunspot numbers from 1749 - 2001 with the help of the wavelet transform .they pointed out that the 154 - 158-day period could be the third harmonic of the 1.3-year ( 475-day ) period .moreover , the both periods fluctuate considerably with time , being stronger during stronger sunspot cycles .therefore , the wavelet analysis suggests a common origin of the both periodicities . this conclusion confirms the de method result which indicates that the periodogram peak at 144 days is an alias of the periodicity from the interval of 1  ]in order to verify the existence of the periodicity at about 155 days i consider the following time series : + 145     + 146   + 147  + the value 134 is calculated analogously to 83 ( see sect .the values 148 and 149 are evaluated from the formula ( 9 ) . in the upper part of figure [ f9 ] the time series of sunspot areas 150 of the one rotation time interval from the whole solar disk and the time series of consecutively smoothed sunspot areas 151are showed . in the lower part of figure [ f9 ]the time series of sunspot area fluctuations 145 is presented .on the basis of these data the maximum activity period of cycle 16 is evaluated .it is an interval between two strongest fluctuations e.a .152  ] rotations .the length of the time interval 153 is 54 rotations .if the about 0-day ( 6 solar rotations ) periodicity existed in this time interval and it was characteristic for strong fluctuations from this time interval , 10 local maxima in the set of 154 would be seen .then it should be necessary to find such a value of p for which 155 for 156 and the number of the local maxima of these values is 10 .as it can be seen in the lower part of figure [ f9 ] this is for the case of 157 ( in this figure the dashed horizontal line is the level of 158 ) .figure [ f10 ] presents nine time distances among the successive fluctuation local maxima and the horizontal line represents the 6-rotation periodicity .it is immediately apparent that the dispersion of these points is 10 and it is difficult to find even few points which oscillate around the value of 6 .such an analysis was carried out for smaller and larger 136 and the results were similar .therefore , the fact , that the about 0-day periodicity exists in the time series of sunspot area fluctuations during the maximum activity period is questionable .    .the horizontal line represents the 6-rotation ( 162-day ) period . ]    ]    ]    to verify again the existence of the about 0-day periodicity during the maximum activity period in each solar hemisphere separately , the time series 88 and 89 were also cut down to the maximum activity period ( january 1925december 1930 ) .the comparison of the autocorrelation functions of these time series with the appriopriate autocorrelation functions of the time series 88 and 89 , which are computed for the whole 11-year cycle ( the lower curves of figures [ f1 ] and [ f2 ] ) , indicates that there are not significant differences between them especially for 23=5 and 6 rotations ( 135 and 162 days ) ) .this conclusion is confirmed by the analysis of the time series 146 for the maximum activity period .the autocorrelation function ( the lower curve of figure [ f11 ] ) is negative for the interval of [ 57 , 173 ] days , but the resolution of the periodogram is too low to find the significant peak at 159 days .the autocorrelation function gives the same result as for daily sunspot area fluctuations from the whole solar disk ( 160 ) ( see also the lower curve of figures [ f5 ] ) . in the case ofthe time series 89 161 is zero for the fluctuations from the whole solar cycle and it is almost zero ( 162 ) for the fluctuations from the maximum activity period .the value 163 is negative .similarly to the case of the northern hemisphere the autocorrelation function and the periodogram of southern hemisphere daily sunspot area fluctuations from the maximum activity period 147 are computed ( see figure [ f12 ] ) .the autocorrelation function has the statistically significant positive peak in the interval of [ 155 , 165 ] days , but the periodogram has too low resolution to decide about the possible periodicities .the correlative analysis indicates that there are positive fluctuations with time distances about 0 days in the maximum activity period .the results of the analyses of the time series of sunspot area fluctuations from the maximum activity period are contradict with the conclusions of  .she uses the power spectrum analysis only .the periodogram of daily sunspot fluctuations contains peaks , which could be harmonics or subharmonics of the true periodicities .they could be treated as real periodicities .this effect is not visible for sunspot data of the one rotation time interval , but averaging could lose true periodicities .this is observed for data from the southern hemisphere .there is the about 0-day peak in the autocorrelation function of daily fluctuations , but the correlation for data of the one rotation interval is almost zero or negative at the points 164 and 6 rotations .thus , it is reasonable to research both time series together using the correlative and the power spectrum analyses .the following results are obtained :    1 .a new method of the detection of statistically significant peaks of the periodograms enables one to identify aliases in the periodogram .2 .   two effects cause the existence of the peak of the periodogram of the time series of sunspot area fluctuations at about 0 days : the first is caused by the 27-day periodicity , which probably creates the 162-day periodicity ( it is a subharmonic frequency of the 27-day periodicity ) and the second is caused by statistically significant positive values of the autocorrelation function from the intervals of 165  ] and 166  ] days .the existence of the periodicity of about 0 days of the time series of sunspot area fluctuations and sunspot area fluctuations from the northern hemisphere during the maximum activity period is questionable .the autocorrelation analysis of the time series of sunspot area fluctuations from the southern hemisphere indicates that the periodicity of about 155 days exists during the maximum activity period .i appreciate valuable comments from professor j. jakimiec .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Прошу прощения за костыли в виде четырех строк ,но не было времени разобраться почему не работает map с такой же логикой\n",
    "\n",
    "patterns = [r'\\n' ,r'\\@xmath', r'\\$', r'\\@xcite']\n",
    "test_clear_article = test_token['article']\n",
    "# test_clear_article = list(map(lambda x: ''.join(\n",
    "#                               re.sub(x,'', test_clear_article)\n",
    "# ), patterns))\n",
    "\n",
    "\n",
    "\n",
    "test_clear_article = re.sub(r'\\n','', test_clear_article)\n",
    "test_clear_article = re.sub(r'@xmath*','', test_clear_article)\n",
    "test_clear_article = re.sub(r'\\$','', test_clear_article)\n",
    "test_clear_article = re.sub(r'@xcite*','', test_clear_article)\n",
    "test_clear_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd6898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    test_clear_article = text\n",
    "    test_clear_article = re.sub(r'\\n','', test_clear_article)\n",
    "    test_clear_article = re.sub(r'@xmath*','', test_clear_article)\n",
    "    test_clear_article = re.sub(r'\\$','', test_clear_article)\n",
    "    test_clear_article = re.sub(r'@xcite*','', test_clear_article)\n",
    "    return test_clear_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05994e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the hybrid monte carlo algorithm ( hmc)@xcite , often used to study quantum chromodynamics ( qcd ) on the lattice , one is interested in efficient numerical time integration schemes which are optimal in terms of computational costs per trajectory for a given acceptance rate . high order\n",
      "numerical methods allow the use of larger step sizes , but demand a larger computational effort per step ; low order schemes do not require such large computational costs per step , but need more steps per trajectory .\n",
      "so there is a need to balance these opposing effects .\n",
      "omelyan integration schemes @xcite of a force - gradient type have proved to be an efficient choice , since it is easy to obtain higher order schemes that demand a small additional computational effort .\n",
      "these schemes use higher - order information from force - gradient terms to both increase the convergence of the method and decrease the size of the leading error coefficient . other ideas to achieve better efficiency for numerical time integrators are given by multirate or nested approaches .\n",
      "these schemes do not increase the order but reduce the computational costs per path by recognizing the different dynamical time - scales generated by different parts of the action .\n",
      "slow forces , which are usually expensive to evaluate , need only to be sampled at low frequency while fast forces which are usually cheap to evaluate need to be sampled at a high frequency . a natural way to inherit the advantages from both force - gradient type schemes and multirate approaches would be to combine these two ideas .    previously , we studied the behavior of the adapted nested force - gradient scheme for the example of the @xmath0-body problem @xcite and would like to learn more about their usefulness for lattice field theory calculations . due to the huge computational effort required for qcd simulations ,\n",
      "it is natural to attempt an intermediate step first .\n",
      "we chose the model problem of quantum electrodynamics ( qed ) in two dimensions , the schwinger model @xcite , since it is well - suited as a test case for new concepts and ideas which can be subsequently applied to more computationally demanding problems @xcite .\n",
      "as a lattice quantum field theory , it has many of the properties of more sophisticated models such as qcd , for example the numerical cost is still dominated by the fermion part of the action . the fact that this model , with far fewer degrees of freedom , does not require such large computational effort makes it the perfect choice for testing purposes .\n",
      "we compare the behavior of numerical time integration schemes currently used for hmc @xcite with the nested force - gradient integrator @xcite and the adapted version introduced in @xcite .\n",
      "we investigate the computational costs needed to perform numerical calculations , as well as the effort required to achieve a satisfactory acceptance rate during the hmc evolution .\n",
      "our goal is to find a numerical scheme for the hmc algorithm which would provide a sufficiently high acceptance rate while not drastically increasing the simulation time .\n",
      "the paper is organized as follows . in section 2\n",
      "we give a short overview of the hmc algorithm and numerical schemes for time integration , which are used in hmc . in section 3\n",
      "we present the 2-dimensional schwinger model and introduce the idea of the force - gradient approach and the resulting novel class of numerical schemes .\n",
      "section 4 is devoted to the results of a comparison between widely used algorithms and the new approach and section 5 draws our conclusion .\n",
      "in this section we provide a general overview of the hmc algorithm @xcite to introduce our novel integrator .\n",
      "we also present some standard numerical time integrating methods used in hmc , as well state - of - the - art numerical schemes , which we later compare by applying them to the two - dimensional schwinger model .      in the hybrid monte carlo algorithm ,\n",
      "the quantum lattice field theory is embedded in a higher - dimensional classical system through the introduction of a fictitious ( simulation ) time @xcite .\n",
      "the gauge field @xmath1 is associated with its ( fictitious ) conjugate momenta @xmath2 , and the classical system is described by the hamiltonian , @xmath3 + { \\mathcal{b}}[u],\\ ] ] where @xmath4 $ ] and @xmath5 $ ] represent the kinetic and potential energy respectively .    for a given configuration @xmath1 ,\n",
      "a new configuration @xmath6 is generated by performing an hmc update @xmath7 , which consists of two steps :    * * molecular dynamics trajectory : * evolve the gauge fields @xmath1 , elements of a lie group , and the momenta @xmath2 , elements of the corresponding lie algebra , in a fictitious computer time @xmath8 according to hamilton s equations of motions @xmath9 since analytical solutions are not available in general , numerical methods must be used to solve the system of eqn .  .\n",
      "the discrete updates of @xmath1 and @xmath2 with an integration step @xmath10 are @xmath11 leading to a first - order approximation at time @xmath12 .\n",
      "since the momenta @xmath2 are elements of lie algebra , we have an additive update of @xmath2 .\n",
      "on the other hand , the links @xmath1 must be elements of the lie group , therefore an exponential update is used for @xmath1 to preserve the underlying group structure . * * metropolis step : * accept or reject the new configuration @xmath13 with probability @xmath14 where @xmath15 .      in this paper\n",
      "we are concerned with numerical time integration schemes , which preserve the fundamental properties of geometric integration , time - reversibility and volume - preservation .\n",
      "all numerical schemes presented below possess these necessary properties .\n",
      "* basic schemes : * well - known , commonly used integration schemes in molecular dynamics are given by    * the leap - frog method , a 3-stage composition scheme of the discrete updates defined above : @xmath16 * and a 5-stage extension widely used in qcd computations : @xmath17    * force gradient schemes : * force - gradient schemes increase accuracy by using additional information from the force gradient term @xmath18 , with @xmath19 defining lie brackets . the 5-stage force - gradient scheme proposed by omelyan et\n",
      "al @xcite is the simplest ; @xmath20 here we also test the modification of the force - gradient method proposed in @xcite , where the force - gradient term @xmath21 is approximated via a taylor expansion .\n",
      "an extension is given by the 11-stage decomposition @xcite , recently implemented as the integrator in the open source code openqcd as one of the standard options @xcite\n",
      "@xmath22where @xmath23 , @xmath24 , @xmath25 and @xmath26 are parameters from equation ( 71 ) in ref .\n",
      "@xcite .    *\n",
      "nested schemes : * qed and qcd problems usually lead to hamiltonians with the following fine structure @xmath27 + { \\mathcal{b}}_{1}[u]+ { \\mathcal{b}}_{2}[u],\\ ] ] where the action of the system can be split into two parts : a fast action @xmath28 such as the gauge action , and a slow part @xmath29 , for example , the fermion action .\n",
      "this allows us to apply the idea of multirate schemes ( an idea known as nested integration in physics literature)@xcite in order to reduce the computational effort . at\n",
      "first we consider the nested version of the leap - frog method @xmath30 where the inner cheaper system @xmath31+{\\mathcal{b}}_{1}[u]$ ] is solved by @xmath32 with @xmath33 being a number of iterations for the fast part of the action .\n",
      "our main goal is to compare the above - mentioned methods with more elaborated nested schemes : in @xcite , a similar 5-stage decomposition scheme has been recently introduced : @xmath34    a nested version of , which has been used in  @xcite reads @xmath35 where @xmath36 with @xmath37 and @xmath38 . in the limit @xmath39\n",
      "we have @xmath40 .\n",
      "note that this approach uses force - gradient information at all levels , i.e. , the high computational cost of high order schemes appears at all levels .\n",
      "one may overcome this problem by using schemes of different order at the different levels without losing the effective high order of the overall multirate scheme .\n",
      "for the latter , we include appropriate force gradient information as we explain in the following for the case of two time levels , where the gauge action plays the role of the fast and cheap part , and the fermionic action plays the role of the slow and expensive part .\n",
      "the reasoning is as follows : if one uses the 5-stage sexton - weingarten integrator of second order for the slow action , and approximates the fast action by @xmath41 leap - frog steps of step size @xmath42 , the error of the overall multirate scheme will be of order @xmath43 . with the use of force gradient information only at the slowest level it is possible to cancel the leading error term of order @xmath44 .\n",
      "as @xmath45 usually holds in the multirate setting , the overall order is then given by the leading error term of order @xmath46 , i.e. , the scheme has an effective order of four .\n",
      "one example for such a scheme for problems of type is given by the 5-stage nested force - gradient scheme introduced in @xcite @xmath47 to summarize , the adapted scheme   differs from the original one   in two perspectives :    * the force gradient scheme for the fast action is replaced by a leap - frog scheme . *\n",
      "only the part @xmath48 of the full force gradient @xmath49 is needed to gain the effective order of four .\n",
      "the numerical schemes - and - are second order convergent schemes . methods - and - have the fourth order of convergence .\n",
      "we do not consider integrators of higher order than four since the computational costs are too high .\n",
      "the schemes of the same convergence order differ from each other by the number of stages ( updates of momenta and links per time step ) .\n",
      "usually methods with more stages have smaller leading error coefficients and therefore have better accuracy , but higher computational costs .\n",
      "we would like to determine which integrator would represent the best compromise between high accuracy and computational efficiency\n",
      ".    we will apply all these numerical integration schemes  to the two - dimensional schwinger model .\n",
      "the most challenging task from the theoretical point of view is to derive the force - gradient term @xmath21 . in the next section\n",
      "we introduce the schwinger model and explain how to obtain the force - gradient term .\n",
      "the 2 dimensional schwinger model is defined by the following hamiltonian function @xmath50 = \\frac{1}{2}\\sum_{n=1,\\mu=1}^{v,2 } p_{n,\\mu}^2 + s_g[u ] + s_f[u].}\\ ] ] with @xmath51 the volume of the lattice . unlike qcd , where @xmath52 and @xmath53 , for this qed problem , the links @xmath1 are the elements of the lie group @xmath54 and the momenta @xmath55 belong to @xmath56 , which represents the lie algebra of the group @xmath54 .\n",
      "this makes this test example very cheap in terms of the computational time .\n",
      "this together with the fact that the schwinger model also shares many of the features of qcd simulations , makes the schwinger model an excellent test example when considering numerical integrators : a fast dynamics given by the computationally cheap gauge part @xmath57 $ ] of the action demanding small step sizes , and a slow dynamics given by the computationally expensive fermion part @xmath58 $ ] allowing large step sizes .\n",
      "the pure gauge part of the action @xmath59 sums up over all plaquettes @xmath60 in the two - dimensional lattice with @xmath61 and is given by @xmath62 the links @xmath1 can be written in the form @xmath63 and connect the sites @xmath0 and @xmath64 on the lattice ; @xmath65 $ ] , @xmath66 , @xmath67 @xmath68 are respectively space and time directions and @xmath69 is a coupling constant .\n",
      "note that from now on we will set the lattice spacing @xmath70 .\n",
      "the fermion part of the action @xmath71 is given by @xmath72 where @xmath26 is a complex pseudofermion field . here , @xmath73 denotes the wilson  dirac operator given by @xmath74 where @xmath75 are the pauli matrices @xmath76 @xmath77 is the mass parameter and the kronecker delta @xmath78 acts on the pseudofermion field by @xmath79 with @xmath80 the pseudofermion field , a vector in the two - dimensional spinor space taking values at each lattice point @xmath0 . in order to proceed with the numerical integration we need to obtain the force @xmath81 and the force gradient term @xmath21 .\n",
      "the force term @xmath82 with respect to the link @xmath83 is given by the first derivative of the action @xmath84 and can be written as @xmath85 since the numerical schemes  use the multi - rate approach , the shifts in the momenta updates are split on @xmath86 and @xmath87 and we can consider them separately .\n",
      "the force terms @xmath86 and @xmath87 are obtained by differentiation over @xmath54 group elements , which for the schwinger model is the standard differentiation .    the force associated with link @xmath88 from the gauge action\n",
      "is given by @xmath89 the force term of the fermion part is given by @xmath90 \\,\\ ] ] where vectors @xmath91 and @xmath92 are given @xmath93    for the numerical methods and we need to find the force gradient term @xmath94 with respect to the link @xmath83 . in case of the schwinger model this term reads @xmath95    for simplicity we decompose the force gradient term in four parts @xmath96 this decomposition is also useful since the numerical integrator only uses the term @xmath97 by construction .\n",
      "as shown in @xcite , to obtain the fourth order convergent scheme from the second order convergent method we must eliminate the leading error term , which is exactly represented by @xmath97 . for completeness we discuss all 4 parts below .\n",
      "the @xmath98 part of the force - gradient term is @xmath99 \\end{aligned}\\ ] ] with the set of plaquettes @xmath100 then by using the vectors @xmath101 defined in we obtain the @xmath102 piece of the force - gradient term given by @xmath103 . }\n",
      "\\end{aligned}\\ ] ] the second derivative of the fermion action is @xmath104 \\xi + } \\nonumber \\\\ & & { 2 \\operatorname{re } \\chi^\\dagger \\frac{\\partial d}{\\partial q_\\mu(n ) } ( d^\\dagger d)^{-1 } \\frac{\\partial d^\\dagger}{\\partial q_\\nu(m ) } \\chi \\ , , } \\nonumber \\\\ & & { = 2 \\operatorname{re } \\left [ z_{1,m,\\nu}^\\dagger \\frac{\\partial d}{\\partial q_{\\mu}(n ) } \\xi +   \\chi^\\dagger\n",
      "\\frac{\\partial d}{\\partial q_{\\mu}(n ) } d^{-1 } w_{2,m,\\nu } -   \\chi^\\dagger   \\frac{\\partial^2 d}{\\partial q_{\\nu}(m ) \\partial q_{\\mu}(n ) } \\xi +   \\chi^\\dagger\n",
      "\\frac{\\partial d}{\\partial q_{\\mu}(n ) } d^{-1 } z_{1,m,\\nu}\\right ] } \\nonumber \\\\ & & { = 2 \\textrm{re } \\left [ z_{1,m,\\nu}^\\dagger w_{2,n,\\mu } +   w_{1,n,\\mu}^\\dagger z_{2,m,\\nu } -   \\chi^\\dagger   \\frac{\\partial^2 d}{\\partial q_{\\nu}(m ) \\partial q_{\\mu}(n ) } \\xi \\right ] } \\end{aligned}\\ ] ] in terms of the vectors @xmath105 and @xmath92 defined in .\n",
      "now the fields @xmath106 and @xmath107 are given by @xmath108 with @xmath109    in order to calculate @xmath110 and @xmath111 it is possible to perform the summation of @xmath112 before the inversions of @xmath73 and @xmath113 to get @xmath114 and @xmath115 which save @xmath116 additional inversions for the force gradient terms .\n",
      "it follows for the force gradient term @xmath111 @xmath117\\ ] ] with @xmath118 + z_1 \\right ) \\ , .\n",
      "\\end{aligned}\\ ] ] the expression for @xmath110 can be obtained from the one for @xmath111 by replacing in and the vector @xmath119 with @xmath120 defined in .\n",
      "it is important to mention that the computationally most demanding part of the numerical integration of the schwinger model and quantum field theory in general is the inverse of the dirac operator @xmath121 .\n",
      "every momenta update , which includes fermion action requires 2 inversions of the dirac operator , the addition of the force - gradient term @xmath21 requires 4 more inversions .\n",
      "therefore leap - frog based methods and need 4 computations of @xmath121 per time step ; schemes and 6 times ; force - gradient based methods 8 for and , 10 for and the 11 stage method has 12 inversions of the dirac operator . since we use the multi - rate approach for schemes , and , which leads generally to fewer macro time steps needed than for the standard schemes we expect the integrator will be the most efficient choice among the methods considered . in the next section we present numerical tests of this prediction .\n",
      "in this section we apply the numerical integrators  to compute the molecular dynamics step for the schwinger model when studied with the hmc algorithm .\n",
      "we consider a @xmath122 by @xmath122 lattice with a coupling constant @xmath123 and mass @xmath124 .\n",
      "the parameters were taken from @xcite and correspond to the scaling variable @xmath125 defined in @xcite.we have chosen them to simulate close to the scaling limit with light fermions and also to increase the impact of the fermion part of the action .\n",
      "we use one thermalised gauge configuration . for each integrator and value of the step - size\n",
      "we generate @xmath126 independent sets of momenta and integrate the equations of motion on a trajectory of length @xmath127 .\n",
      "we compute the absolute error @xmath128 and estimate its statistical error from the standard deviation .\n",
      "also the parameter @xmath33 is chosen in such a way to make micro step size to be @xmath129 times smaller than the macro step size @xmath10 .\n",
      "figure [ fig:1 ] presents the comparison between the numerical integrators  .\n",
      "it shows the absolute error @xmath128 versus the step - size of the numerical scheme . here\n",
      "the multi - rate schemes , , and outperform their standard versions as expected .\n",
      "also it is easy to see that the scheme has the best accuracy and the nested force - gradient method just slightly edges the adapted nested force - gradient scheme .\n",
      "figure [ fig:2 ] presents the cpu time , required for the proposed integrators \n",
      ", versus the achieved accuracy .\n",
      "we can observe that the nested force - gradient method and adapted nested force- gradient method show much better results in terms of a computational efficiency than the integrators and ; and even compared to the 11 stage scheme .\n",
      "here we can see that the modification of proposed in @xcite also performs better than its original version .\n",
      "it shows almost similar computational costs as nested versions of the force - gradient approach - , since it has the same number of @xmath121  ( see table [ tab:1 ] ) .\n",
      "but it is less efficient because the proposed nested approach is more precise .\n",
      ".step - sizes and number of inversions of @xmath73 per step and per trajectory for acceptance rate of 90% [ cols=\"^,^,^,^,^\",options=\"header \" , ]     table [ tab:1 ] shows the number of inversions of the dirac operator @xmath73 , which is needed to reach 90% acceptance rate of the hmc . since @xmath121 is the most computationally demanding part it is important to see how many of these inversions are required per each trajectory . from table\n",
      "[ tab:1 ] it easy to see that the adapted nested force - gradient method and nested force - gradient method need the least number of @xmath121 per trajectory to reach the chosen acceptance rate @xmath130 .\n",
      "we can also claim that methods and have a potential to perform even better with respect to the computational effort in the case of lattice qcd problems , since the impact of the fermion action and the computational time to obtain the inversion of the dirac operator @xmath73 is much more significant .\n",
      "we presented the nested force - gradient approach and its adapted version applied to a model problem in quantum field theory , the two - dimensional schwinger model .\n",
      "the derivation of the force - gradient terms was given and the schwinger model was introduced .\n",
      "nested force - gradient schemes seem to be an optimal choice with relatively high convergence order and low computational effort .\n",
      "also it would be possible to improve the algorithm by measuring the poisson brackets of the shadow hamiltonian of the proposed integrator and then tuning the set of optimal parameters , e.  g. micro and macro step sizes .\n",
      "+ in future work we will apply this approach to the hmc algorithm for numerical integration in lattice qcd . here\n",
      "we expect the adapted nested - force gradient scheme to outperform the original one , if we further partition the action into more than two parts , by using techniques to factorize the fermion determinant : less force - gradient information is needed for the most expensive action , and only leap - frog steps are needed for the high frequency parts of the action .\n",
      "this work is part of project b5 within the sfb / transregio 55 _ hadronenphysik mit gitter - qcd _ funded by dfg ( deutsche forschungsgemeinschaft ) .\n",
      "s.  duane , a.d .\n",
      "kennedy , b.j .\n",
      "pendleton , d.  roweth , hybrid monte carlo , phys .\n",
      "b195 ( 1987 ) , pp .\n",
      "e.  hairer , c.  lubich , g.  wanner , geometric numerical integration : structure - preserving algorithms for ordinary differential equations , springer , berlin , 2002 .\n",
      "omelyan , i.m .\n",
      "mryglod , r.  folk , symplectic analytically integrable decomposition algorithms : classification , derivation , and application to molecular dynamics , quantum and celestial mechanics , comput .\n",
      "151 ( 2003 ) , pp .\n",
      " we study a novel class of numerical integrators , the adapted nested force - gradient schemes , used within the molecular dynamics step of the hybrid monte carlo ( hmc ) algorithm . \n",
      " we test these methods in the schwinger model on the lattice , a well known benchmark problem . \n",
      " we derive the analytical basis of nested force - gradient type methods and demonstrate the advantage of the proposed approach , namely reduced computational costs compared with other numerical integration schemes in hmc . \n",
      "introduction\n",
      "geometric integrators for hmc\n",
      "the schwinger model and its force gradient terms\n",
      "numerical results\n",
      "conclusions and outlook\n",
      "acknowledgments\n"
     ]
    }
   ],
   "source": [
    "y = next(test_iter)\n",
    "for x in y:\n",
    "    print(y[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db89158c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Проверяю работает ли все\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m text_test \u001b[38;5;241m=\u001b[39m \u001b[43mclear_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m text_test\n",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m, in \u001b[0;36mclear_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclear_text\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     test_clear_article \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m----> 3\u001b[0m     test_clear_article \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_clear_article\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test_clear_article \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@xmath*\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, test_clear_article)\n\u001b[0;32m      5\u001b[0m     test_clear_article \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, test_clear_article)\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\re\\__init__.py:185\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'dict'"
     ]
    }
   ],
   "source": [
    "# Проверяю работает ли все\n",
    "text_test = clear_text(test_token['article'])\n",
    "text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ee12361",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "456fabe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for',\n",
       " 'about',\n",
       " '20',\n",
       " 'years',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'short',\n",
       " '-',\n",
       " 'term',\n",
       " 'changes',\n",
       " 'of',\n",
       " 'solar',\n",
       " 'activity',\n",
       " 'has',\n",
       " 'been',\n",
       " 'considered',\n",
       " 'extensively',\n",
       " '.',\n",
       " 'many',\n",
       " 'investigators',\n",
       " 'studied',\n",
       " 'the',\n",
       " 'short',\n",
       " '-',\n",
       " 'term',\n",
       " 'periodicities',\n",
       " 'of',\n",
       " 'the',\n",
       " 'various',\n",
       " 'indices',\n",
       " 'of',\n",
       " 'solar',\n",
       " 'activity',\n",
       " '.',\n",
       " 'several',\n",
       " 'periodicities',\n",
       " 'were',\n",
       " 'detected',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'periodicities',\n",
       " 'about',\n",
       " '155',\n",
       " 'days',\n",
       " 'and',\n",
       " 'from',\n",
       " 'the',\n",
       " 'interval',\n",
       " 'of',\n",
       " '3',\n",
       " ']',\n",
       " 'days',\n",
       " '(',\n",
       " '4',\n",
       " ']',\n",
       " 'years',\n",
       " ')',\n",
       " 'are',\n",
       " 'mentioned',\n",
       " 'most',\n",
       " 'often',\n",
       " '.',\n",
       " 'first',\n",
       " 'of',\n",
       " 'them',\n",
       " 'was',\n",
       " 'discovered',\n",
       " 'by',\n",
       " 'in',\n",
       " 'the',\n",
       " 'occurence',\n",
       " 'rate',\n",
       " 'of',\n",
       " 'gamma',\n",
       " '-',\n",
       " 'ray',\n",
       " 'flares',\n",
       " 'detected',\n",
       " 'by',\n",
       " 'the',\n",
       " 'gamma',\n",
       " '-',\n",
       " 'ray',\n",
       " 'spectrometer',\n",
       " 'aboard',\n",
       " 'the',\n",
       " '_',\n",
       " 'solar',\n",
       " 'maximum',\n",
       " 'mission',\n",
       " '(',\n",
       " 'smm',\n",
       " ')',\n",
       " '.',\n",
       " 'this',\n",
       " 'periodicity',\n",
       " 'was',\n",
       " 'confirmed',\n",
       " 'for',\n",
       " 'other',\n",
       " 'solar',\n",
       " 'flares',\n",
       " 'data',\n",
       " 'and',\n",
       " 'for',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'period',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'also',\n",
       " 'found',\n",
       " 'in',\n",
       " 'proton',\n",
       " 'flares',\n",
       " 'during',\n",
       " 'solar',\n",
       " 'cycles',\n",
       " '19',\n",
       " 'and',\n",
       " '20',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'not',\n",
       " 'found',\n",
       " 'in',\n",
       " 'the',\n",
       " 'solar',\n",
       " 'flares',\n",
       " 'data',\n",
       " 'during',\n",
       " 'solar',\n",
       " 'cycles',\n",
       " '22',\n",
       " '.',\n",
       " '_',\n",
       " 'several',\n",
       " 'autors',\n",
       " 'confirmed',\n",
       " 'above',\n",
       " 'results',\n",
       " 'for',\n",
       " 'the',\n",
       " 'daily',\n",
       " 'sunspot',\n",
       " 'area',\n",
       " 'data',\n",
       " '.',\n",
       " 'studied',\n",
       " 'the',\n",
       " 'sunspot',\n",
       " 'data',\n",
       " 'from',\n",
       " '18741984',\n",
       " '.',\n",
       " 'she',\n",
       " 'found',\n",
       " 'the',\n",
       " '155-day',\n",
       " 'periodicity',\n",
       " 'in',\n",
       " 'data',\n",
       " 'records',\n",
       " 'from',\n",
       " '31',\n",
       " 'years',\n",
       " '.',\n",
       " 'this',\n",
       " 'periodicity',\n",
       " 'is',\n",
       " 'always',\n",
       " 'characteristic',\n",
       " 'for',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'solar',\n",
       " 'hemispheres',\n",
       " '(',\n",
       " 'the',\n",
       " 'southern',\n",
       " 'hemisphere',\n",
       " 'for',\n",
       " 'cycles',\n",
       " '1215',\n",
       " 'and',\n",
       " 'the',\n",
       " 'northern',\n",
       " 'hemisphere',\n",
       " 'for',\n",
       " 'cycles',\n",
       " '1621',\n",
       " ')',\n",
       " '.',\n",
       " 'moreover',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'only',\n",
       " 'present',\n",
       " 'during',\n",
       " 'epochs',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'activity',\n",
       " '(',\n",
       " 'in',\n",
       " 'episodes',\n",
       " 'of',\n",
       " '13',\n",
       " 'years',\n",
       " ')',\n",
       " '.',\n",
       " 'similarinvestigationswerecarriedoutby',\n",
       " '+',\n",
       " '.',\n",
       " 'they',\n",
       " 'applied',\n",
       " 'the',\n",
       " 'same',\n",
       " 'power',\n",
       " 'spectrum',\n",
       " 'method',\n",
       " 'as',\n",
       " 'lean',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'daily',\n",
       " 'sunspot',\n",
       " 'area',\n",
       " 'data',\n",
       " '(',\n",
       " 'cycles',\n",
       " '1221',\n",
       " ')',\n",
       " 'were',\n",
       " 'divided',\n",
       " 'into',\n",
       " '10',\n",
       " 'shorter',\n",
       " 'time',\n",
       " 'series',\n",
       " '.',\n",
       " 'the',\n",
       " 'periodicities',\n",
       " 'were',\n",
       " 'searched',\n",
       " 'for',\n",
       " 'the',\n",
       " 'frequency',\n",
       " 'interval',\n",
       " '57115',\n",
       " 'nhz',\n",
       " '(',\n",
       " '100200',\n",
       " 'days',\n",
       " ')',\n",
       " 'and',\n",
       " 'for',\n",
       " 'each',\n",
       " 'of',\n",
       " '10',\n",
       " 'time',\n",
       " 'series',\n",
       " '.',\n",
       " 'the',\n",
       " 'authors',\n",
       " 'showed',\n",
       " 'that',\n",
       " 'the',\n",
       " 'periodicity',\n",
       " 'between',\n",
       " '150160',\n",
       " 'days',\n",
       " 'is',\n",
       " 'statistically',\n",
       " 'significant',\n",
       " 'during',\n",
       " 'all',\n",
       " 'cycles',\n",
       " 'from',\n",
       " '16',\n",
       " 'to',\n",
       " '21',\n",
       " '.',\n",
       " 'the',\n",
       " 'considered',\n",
       " 'peaks',\n",
       " 'were',\n",
       " 'remained',\n",
       " 'unaltered',\n",
       " 'after',\n",
       " 'removing',\n",
       " 'the',\n",
       " '11-year',\n",
       " 'cycle',\n",
       " 'and',\n",
       " 'applying',\n",
       " 'the',\n",
       " 'power',\n",
       " 'spectrum',\n",
       " 'analysis',\n",
       " '.',\n",
       " 'used',\n",
       " 'the',\n",
       " 'wavelet',\n",
       " 'technique',\n",
       " 'for',\n",
       " 'the',\n",
       " 'daily',\n",
       " 'sunspot',\n",
       " 'areas',\n",
       " 'between',\n",
       " '1874',\n",
       " 'and',\n",
       " '1993',\n",
       " '.',\n",
       " 'they',\n",
       " 'determined',\n",
       " 'the',\n",
       " 'epochs',\n",
       " 'of',\n",
       " 'appearance',\n",
       " 'of',\n",
       " 'this',\n",
       " 'periodicity',\n",
       " 'and',\n",
       " 'concluded',\n",
       " 'that',\n",
       " 'it',\n",
       " 'presents',\n",
       " 'around',\n",
       " 'the',\n",
       " 'maximum',\n",
       " 'activity',\n",
       " 'period',\n",
       " 'in',\n",
       " 'cycles',\n",
       " '16',\n",
       " 'to',\n",
       " '21',\n",
       " '.',\n",
       " 'moreover',\n",
       " ',',\n",
       " 'the',\n",
       " 'power',\n",
       " 'of',\n",
       " 'this',\n",
       " 'periodicity',\n",
       " 'started',\n",
       " 'growing',\n",
       " 'at',\n",
       " 'cycle',\n",
       " '19',\n",
       " ',',\n",
       " 'decreased',\n",
       " 'in',\n",
       " 'cycles',\n",
       " '20',\n",
       " 'and',\n",
       " '21',\n",
       " 'and',\n",
       " 'disappered',\n",
       " 'after',\n",
       " 'cycle',\n",
       " '21',\n",
       " '.',\n",
       " 'similaranalyseswerepresentedby',\n",
       " '+',\n",
       " ',',\n",
       " 'but',\n",
       " 'for',\n",
       " 'sunspot',\n",
       " 'number',\n",
       " ',',\n",
       " 'solar',\n",
       " 'wind',\n",
       " 'plasma',\n",
       " ',',\n",
       " 'interplanetary',\n",
       " 'magnetic',\n",
       " 'field',\n",
       " 'and',\n",
       " 'geomagnetic',\n",
       " 'activity',\n",
       " 'index',\n",
       " '5',\n",
       " '.',\n",
       " 'during',\n",
       " '1964',\n",
       " '-',\n",
       " '2000',\n",
       " 'the',\n",
       " 'sunspot',\n",
       " 'number',\n",
       " 'wavelet',\n",
       " 'power',\n",
       " 'of',\n",
       " 'periods',\n",
       " 'less',\n",
       " 'than',\n",
       " 'one',\n",
       " 'year',\n",
       " 'shows',\n",
       " 'a',\n",
       " 'cyclic',\n",
       " 'evolution',\n",
       " 'with',\n",
       " 'the',\n",
       " 'phase',\n",
       " 'of',\n",
       " 'the',\n",
       " 'solar',\n",
       " 'cycle',\n",
       " '.',\n",
       " 'the',\n",
       " '154-day',\n",
       " 'period',\n",
       " 'is',\n",
       " 'prominent',\n",
       " 'and',\n",
       " 'its',\n",
       " 'strenth',\n",
       " 'is',\n",
       " 'stronger',\n",
       " 'around',\n",
       " 'the',\n",
       " '1982',\n",
       " '-',\n",
       " '1984',\n",
       " 'interval',\n",
       " 'in',\n",
       " 'almost',\n",
       " 'all',\n",
       " 'solar',\n",
       " 'wind',\n",
       " 'parameters',\n",
       " '.',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'the',\n",
       " '156-day',\n",
       " 'periodicity',\n",
       " 'in',\n",
       " 'sunspot',\n",
       " 'data',\n",
       " 'were',\n",
       " 'confirmed',\n",
       " 'by',\n",
       " '.',\n",
       " 'they',\n",
       " 'considered',\n",
       " 'the',\n",
       " 'possible',\n",
       " 'relation',\n",
       " 'between',\n",
       " 'the',\n",
       " '475-day',\n",
       " '(',\n",
       " '1',\n",
       " '.',\n",
       " '3-year',\n",
       " ')',\n",
       " 'and',\n",
       " '156-day',\n",
       " 'periodicities',\n",
       " '.',\n",
       " 'the',\n",
       " '475-day',\n",
       " '(',\n",
       " '1',\n",
       " '.',\n",
       " '3-year',\n",
       " ')',\n",
       " 'periodicity',\n",
       " 'was',\n",
       " 'also',\n",
       " 'detected',\n",
       " 'in',\n",
       " 'variations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'interplanetary',\n",
       " 'magnetic',\n",
       " 'field',\n",
       " ',',\n",
       " 'geomagnetic',\n",
       " 'activity',\n",
       " 'helioseismic',\n",
       " 'data',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'solar',\n",
       " 'wind',\n",
       " 'speed',\n",
       " '.',\n",
       " 'concluded',\n",
       " 'that',\n",
       " 'the',\n",
       " 'region',\n",
       " 'of',\n",
       " 'larger',\n",
       " 'wavelet',\n",
       " 'power',\n",
       " 'shifts',\n",
       " 'from',\n",
       " '475-day',\n",
       " '(',\n",
       " '1',\n",
       " '.',\n",
       " '3-year',\n",
       " ')',\n",
       " 'period',\n",
       " 'to',\n",
       " '620-day',\n",
       " '(',\n",
       " '1',\n",
       " '.',\n",
       " '7-year',\n",
       " ')',\n",
       " 'period',\n",
       " 'and',\n",
       " 'then',\n",
       " 'back',\n",
       " 'to',\n",
       " '475-day',\n",
       " '(',\n",
       " '1',\n",
       " '.',\n",
       " '3-year',\n",
       " ')',\n",
       " '.',\n",
       " 'the',\n",
       " 'periodicities',\n",
       " 'from',\n",
       " 'the',\n",
       " 'interval',\n",
       " '6',\n",
       " ']',\n",
       " 'days',\n",
       " '(',\n",
       " '4',\n",
       " ']',\n",
       " 'years',\n",
       " ')',\n",
       " 'have',\n",
       " 'been',\n",
       " 'considered',\n",
       " 'from',\n",
       " '1968',\n",
       " '.',\n",
       " 'mentioned',\n",
       " 'a',\n",
       " '16',\n",
       " '.',\n",
       " '3-month',\n",
       " '(',\n",
       " '490-day',\n",
       " ')',\n",
       " 'periodicity',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sunspot',\n",
       " 'numbers',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'geomagnetic',\n",
       " 'data',\n",
       " '.',\n",
       " 'analysed',\n",
       " 'the',\n",
       " 'occurrence',\n",
       " 'rate',\n",
       " 'of',\n",
       " 'major',\n",
       " 'flares',\n",
       " 'during',\n",
       " 'solar',\n",
       " 'cycles',\n",
       " '19',\n",
       " '.',\n",
       " 'they',\n",
       " 'found',\n",
       " 'a',\n",
       " '18-month',\n",
       " '(',\n",
       " '540-day',\n",
       " ')',\n",
       " 'periodicity',\n",
       " 'in',\n",
       " 'flare',\n",
       " 'rate',\n",
       " 'of',\n",
       " 'the',\n",
       " 'norhern',\n",
       " 'hemisphere',\n",
       " '.',\n",
       " 'confirmed',\n",
       " 'this',\n",
       " 'result',\n",
       " 'for',\n",
       " 'the',\n",
       " '7',\n",
       " 'flare',\n",
       " 'data',\n",
       " 'for',\n",
       " 'solar',\n",
       " 'cycles',\n",
       " '20',\n",
       " 'and',\n",
       " '21',\n",
       " 'and',\n",
       " 'found',\n",
       " 'a',\n",
       " 'peak',\n",
       " 'in',\n",
       " 'the',\n",
       " 'power',\n",
       " 'spectra',\n",
       " 'near',\n",
       " '510540',\n",
       " 'days',\n",
       " '.',\n",
       " 'found',\n",
       " 'a',\n",
       " '17-month',\n",
       " '(',\n",
       " '510-day',\n",
       " ')',\n",
       " 'periodicity',\n",
       " 'of',\n",
       " 'sunspot',\n",
       " 'groups',\n",
       " 'and',\n",
       " 'their',\n",
       " 'areas',\n",
       " 'from',\n",
       " '1969',\n",
       " 'to',\n",
       " '1986',\n",
       " '.',\n",
       " 'these',\n",
       " 'authors',\n",
       " 'concluded',\n",
       " 'that',\n",
       " 'the',\n",
       " 'length',\n",
       " 'of',\n",
       " 'this',\n",
       " 'period',\n",
       " 'is',\n",
       " 'variable',\n",
       " 'and',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " 'periodicity',\n",
       " 'is',\n",
       " 'still',\n",
       " 'not',\n",
       " 'understood',\n",
       " '.',\n",
       " 'and',\n",
       " '+',\n",
       " 'obtained',\n",
       " 'statistically',\n",
       " 'significant',\n",
       " 'peaks',\n",
       " 'of',\n",
       " 'power',\n",
       " 'at',\n",
       " 'around',\n",
       " '158',\n",
       " 'days',\n",
       " 'for',\n",
       " 'daily',\n",
       " 'sunspot',\n",
       " 'data',\n",
       " 'from',\n",
       " '1923',\n",
       " '-',\n",
       " '1933',\n",
       " '(',\n",
       " 'cycle',\n",
       " '16',\n",
       " ')',\n",
       " '.',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'this',\n",
       " 'periodicity',\n",
       " 'for',\n",
       " 'sunspot',\n",
       " 'data',\n",
       " 'from',\n",
       " 'cycle',\n",
       " '16',\n",
       " 'is',\n",
       " 'considered',\n",
       " '.',\n",
       " 'the',\n",
       " 'daily',\n",
       " 'sunspot',\n",
       " 'areas',\n",
       " ',',\n",
       " 'the',\n",
       " 'mean',\n",
       " 'sunspot',\n",
       " 'areas',\n",
       " 'per',\n",
       " 'carrington',\n",
       " 'rotation',\n",
       " ',',\n",
       " 'the',\n",
       " 'monthly',\n",
       " 'sunspot',\n",
       " 'numbers',\n",
       " 'and',\n",
       " 'their',\n",
       " 'fluctuations',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'obtained',\n",
       " 'after',\n",
       " 'removing',\n",
       " 'the',\n",
       " '11-year',\n",
       " 'cycle',\n",
       " 'are',\n",
       " 'analysed',\n",
       " '.',\n",
       " 'in',\n",
       " 'section',\n",
       " '2',\n",
       " 'the',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'the',\n",
       " 'power',\n",
       " 'spectrum',\n",
       " 'methods',\n",
       " 'are',\n",
       " 'described',\n",
       " '.',\n",
       " 'in',\n",
       " 'section',\n",
       " '3',\n",
       " 'a',\n",
       " 'new',\n",
       " 'approach',\n",
       " 'to',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'aliases',\n",
       " 'in',\n",
       " 'the',\n",
       " 'power',\n",
       " 'spectrum',\n",
       " 'analysisis',\n",
       " 'presented',\n",
       " '.',\n",
       " 'in',\n",
       " 'section',\n",
       " '4',\n",
       " 'numerical',\n",
       " 'results',\n",
       " 'of',\n",
       " 'the',\n",
       " 'new',\n",
       " 'method',\n",
       " 'of',\n",
       " 'the',\n",
       " 'diagnosis',\n",
       " 'of',\n",
       " 'an',\n",
       " 'echo',\n",
       " '-',\n",
       " 'effect',\n",
       " 'for',\n",
       " 'sunspot',\n",
       " 'area',\n",
       " 'data',\n",
       " 'are',\n",
       " 'discussed',\n",
       " '.',\n",
       " 'in',\n",
       " 'section',\n",
       " '5',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'periodicity',\n",
       " 'of',\n",
       " 'about',\n",
       " '155',\n",
       " 'days',\n",
       " 'during',\n",
       " 'the',\n",
       " 'maximum',\n",
       " 'activity',\n",
       " 'period',\n",
       " 'for',\n",
       " 'sunspot',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'solar',\n",
       " 'disk',\n",
       " 'and',\n",
       " 'from',\n",
       " 'each',\n",
       " 'solar',\n",
       " 'hemisphere',\n",
       " 'separately',\n",
       " 'is',\n",
       " 'considered',\n",
       " '.',\n",
       " 'to',\n",
       " 'find',\n",
       " 'periodicities',\n",
       " 'in',\n",
       " 'a',\n",
       " 'given',\n",
       " 'time',\n",
       " 'series',\n",
       " 'the',\n",
       " 'power',\n",
       " 'spectrum',\n",
       " 'analysis',\n",
       " 'is',\n",
       " 'applied',\n",
       " '.',\n",
       " 'in',\n",
       " 'this',\n",
       " 'papertwo',\n",
       " 'methods',\n",
       " 'are',\n",
       " 'used',\n",
       " 'the',\n",
       " 'fast',\n",
       " 'fourier',\n",
       " 'transformation',\n",
       " 'algorithm',\n",
       " 'with',\n",
       " 'the',\n",
       " 'hamming',\n",
       " 'window',\n",
       " 'function',\n",
       " '(',\n",
       " 'fft',\n",
       " ')',\n",
       " 'and',\n",
       " 'the',\n",
       " 'blackman',\n",
       " '-',\n",
       " 'tukey',\n",
       " '(',\n",
       " 'bt',\n",
       " ')',\n",
       " 'power',\n",
       " 'spectrum',\n",
       " 'method',\n",
       " '.',\n",
       " 'the',\n",
       " 'bt',\n",
       " 'method',\n",
       " 'is',\n",
       " 'used',\n",
       " 'for',\n",
       " 'the',\n",
       " 'diagnosis',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reasons',\n",
       " 'of',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'peaks',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'obtained',\n",
       " 'by',\n",
       " 'the',\n",
       " 'fft',\n",
       " 'method',\n",
       " '.',\n",
       " 'the',\n",
       " 'bt',\n",
       " 'method',\n",
       " 'consists',\n",
       " 'in',\n",
       " 'the',\n",
       " 'smoothing',\n",
       " 'of',\n",
       " 'a',\n",
       " 'cosine',\n",
       " 'transform',\n",
       " 'of',\n",
       " 'an',\n",
       " 'autocorrelation',\n",
       " 'function',\n",
       " 'using',\n",
       " 'a',\n",
       " '3-point',\n",
       " 'weighting',\n",
       " 'average',\n",
       " '.',\n",
       " 'such',\n",
       " 'an',\n",
       " 'estimator',\n",
       " 'is',\n",
       " 'consistent',\n",
       " 'and',\n",
       " 'unbiased',\n",
       " '.',\n",
       " 'moreover',\n",
       " ',',\n",
       " 'the',\n",
       " 'peaks',\n",
       " 'are',\n",
       " 'uncorrelated',\n",
       " 'and',\n",
       " 'their',\n",
       " 'sum',\n",
       " 'is',\n",
       " 'a',\n",
       " 'variance',\n",
       " 'of',\n",
       " 'a',\n",
       " 'considered',\n",
       " 'time',\n",
       " 'series',\n",
       " '.',\n",
       " 'the',\n",
       " 'main',\n",
       " 'disadvantage',\n",
       " 'of',\n",
       " 'this',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'weak',\n",
       " 'resolution',\n",
       " 'of',\n",
       " 'the',\n",
       " 'periodogram',\n",
       " 'points',\n",
       " ',',\n",
       " 'particularly',\n",
       " 'for',\n",
       " 'low',\n",
       " 'frequences',\n",
       " '.',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'if',\n",
       " 'the',\n",
       " 'autocorrelation',\n",
       " 'function',\n",
       " 'is',\n",
       " 'evaluated',\n",
       " 'for',\n",
       " '8',\n",
       " ',',\n",
       " 'then',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'points',\n",
       " 'in',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_test_token = tokenizer(str(test_clear_article))\n",
    "res_test_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eacd494",
   "metadata": {},
   "source": [
    "Соединяю два текста (статья и абстракт) в один, чищу его и после посылю в токенизатор. \n",
    "UPD. Была проблема с тем, что на вход приходил генератор, но к ним нельзя обращаться по индексу, поэтому я приписал \n",
    " y = next(data_iter) которые выводит dict с ключами. Возможно это ошибка, но это лучшее что я придумал в 23 часа. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cc3463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    y = next(data_iter)\n",
    "    text = y['article'] +' '+ y['abstract']\n",
    "    clear_article_text = clear_text(text)\n",
    "    yield tokenizer(clear_article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d31b6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4eefb29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30f86045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[305, 8, 59, 0, 745]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['not', 'in', 'for', 'green', 'monte'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88c903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
